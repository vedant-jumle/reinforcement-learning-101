{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delivery_drone.game.socket_client import DroneGameClient, DroneState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DroneGameClient()\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_array(state):\n",
    "    \"\"\"Convert DroneState dataclass to numpy array\"\"\"\n",
    "    data = np.array([\n",
    "        state.drone_x,\n",
    "        state.drone_y,\n",
    "        state.drone_vx,\n",
    "        state.drone_vy,\n",
    "        state.drone_angle,\n",
    "        state.drone_angular_vel,\n",
    "        state.drone_fuel,\n",
    "        state.platform_x,\n",
    "        state.platform_y,\n",
    "        state.distance_to_platform,\n",
    "        state.dx_to_platform,\n",
    "        state.dy_to_platform,\n",
    "        state.speed,\n",
    "        float(state.landed),\n",
    "        float(state.crashed)\n",
    "    ])\n",
    "    \n",
    "    return torch.tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8978106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scaler(value, scaler):\n",
    "    # Does value * scaler\n",
    "    return value * scaler\n",
    "\n",
    "def gausian_scaler(value, sigma=0.1, scaler=1):\n",
    "    return scaler * math.exp(-value**2/(2*sigma**2))\n",
    "\n",
    "def exponential_decay(value, decay=10, scaler=1):\n",
    "    return scaler * math.exp(-decay*(abs(value)))\n",
    "\n",
    "def inverse_quadratic(value, decay=10, scaler=1):\n",
    "    return scaler * (1/(1+(decay*(value**2))))\n",
    "\n",
    "def inverse_linear(value, decay=10, scaler=1):\n",
    "    return scaler * (1/(1+(decay*abs(value))))\n",
    "\n",
    "def scaled_shifted_negative_sigmoid(value, sigma=10, scaler=1):\n",
    "    return scaler * (1/(1+math.exp(sigma*(value-0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e084194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_velocity_alignment(state: DroneState):\n",
    "    \"\"\"\n",
    "    Calculate how well aligned the drone's velocity is with the optimal direction.\n",
    "    Returns cosine similarity: 1.0 = perfect alignment, -1.0 = opposite direction, 0 = perpendicular\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Velocity vector\n",
    "    vx = state.drone_vx\n",
    "    vy = state.drone_vy\n",
    "    \n",
    "    # Direction vector from drone to platform\n",
    "    dx = state.dx_to_platform\n",
    "    dy = state.dy_to_platform\n",
    "    \n",
    "    # Calculate magnitudes\n",
    "    velocity_magnitude = math.sqrt(vx**2 + vy**2)\n",
    "    direction_magnitude = math.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if velocity_magnitude == 0:\n",
    "        return 0  # Stationary drone, no alignment\n",
    "    \n",
    "    if direction_magnitude == 0:\n",
    "        return 0  # Already at platform\n",
    "    \n",
    "    # Dot product\n",
    "    dot_product = vx * dx + vy * dy\n",
    "    \n",
    "    # Cosine similarity: cos(θ) = (v · d) / (|v| * |d|)\n",
    "    cos_theta = dot_product / (velocity_magnitude * direction_magnitude)\n",
    "    \n",
    "    return cos_theta\n",
    "\n",
    "def calc_reward(state: DroneState):\n",
    "    rewards = {}\n",
    "    total_reward = 0\n",
    "    \n",
    "    time_step = state.steps\n",
    "    \n",
    "    # Time penalty\n",
    "    minimum_time_penalty = 0.3\n",
    "    maximum_time_penalty = 1\n",
    "    rewards['time_penalty'] = -inverse_quadratic(\n",
    "        state.distance_to_platform, \n",
    "        decay=50, \n",
    "        scaler=maximum_time_penalty-minimum_time_penalty) - minimum_time_penalty\n",
    "    \n",
    "    # Distance-based time penalty\n",
    "    # Penalty gets smaller as drone gets closer to platform\n",
    "    # Uses inverse quadratic function: higher penalty when far, reduces as distance decreases\n",
    "    # Minimum penalty of 0.5, maximum of 2.0 per timestep\n",
    "    total_reward += rewards['time_penalty']\n",
    "    \n",
    "    velocity_alignment = calc_velocity_alignment(state)\n",
    "    dist = state.distance_to_platform\n",
    "    \n",
    "    rewards['distance'] = 0\n",
    "    rewards['velocity_alignment'] = 0\n",
    "\n",
    "    if dist > 0.065 and state.dy_to_platform > 0:  # ADD: only if drone ABOVE platform\n",
    "        rewards['distance'] = int(velocity_alignment > 0) * state.speed * scaled_shifted_negative_sigmoid(dist, scaler=4.5)\n",
    "        \n",
    "        if velocity_alignment > 0:\n",
    "            rewards['velocity_alignment'] = 0.5\n",
    "\n",
    "    total_reward += rewards['distance']\n",
    "    total_reward += rewards['velocity_alignment']\n",
    "    \n",
    "    # Angle penalty (define a distance based max threshold)\n",
    "    abs_angle = abs(state.drone_angle)\n",
    "    max_angle = 0.20\n",
    "    max_permissible_angle = ((max_angle-0.111)*dist) + 0.111\n",
    "    excess = abs_angle - max_permissible_angle # excess angle\n",
    "    rewards['angle'] = -max(excess, 0) # maximum reward is 0 (we dont want it to reward hack for stability)\n",
    "    \n",
    "    total_reward += rewards['angle']\n",
    "    \n",
    "    # Speed - penalize excessive speed\n",
    "    rewards['speed'] = 0\n",
    "    speed = state.speed\n",
    "    max_speed = 0.4\n",
    "    if dist < 1:\n",
    "        rewards['speed'] = -2 * max(speed-0.1, 0)\n",
    "    else:\n",
    "        rewards['speed'] = -1 * max(speed-max_speed, 0)\n",
    "    total_reward += rewards['speed']\n",
    "    \n",
    "    # Penalize being below platform\n",
    "    rewards['vertical_position'] = 0\n",
    "    if state.dy_to_platform > 0:  # Platform is below drone (drone is above - GOOD)\n",
    "        rewards['vertical_position'] = 0\n",
    "    else:  # Drone is below platform (BAD!)\n",
    "        rewards['vertical_position'] = state.dy_to_platform * 4.0  # Negative penalty\n",
    "    total_reward += rewards['vertical_position']\n",
    "    \n",
    "    # Terminal\n",
    "    rewards['terminal'] = 0\n",
    "    if state.landed:\n",
    "        rewards['terminal'] = 500.0 + state.drone_fuel * 100.0\n",
    "    elif state.crashed:\n",
    "        rewards['terminal'] = -200.0\n",
    "        # Extra penalty for crashing far from target\n",
    "        if state.distance_to_platform > 0.3:\n",
    "            rewards['terminal'] -= 100.0\n",
    "    total_reward += rewards['terminal']\n",
    "    \n",
    "    rewards['total'] = total_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba54e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(client, policy, max_steps=300, game_id=0, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate policy on a single game without training.\n",
    "    \n",
    "    Args:\n",
    "        client (DroneGameClient): Game client instance\n",
    "        policy (DroneGamerBoi): Policy network\n",
    "        max_steps (int): Maximum steps per episode\n",
    "        game_id (int): ID of the game to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Episode statistics including rewards, steps, and outcome\n",
    "    \"\"\"\n",
    "    # Set policy to evaluation mode\n",
    "    policy.eval()\n",
    "    \n",
    "    # Initialize episode\n",
    "    state = client.reset(game_id)\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Run episode\n",
    "    while not done and steps < max_steps:\n",
    "        # Get action probabilities from policy\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            action_probs = policy(state)\n",
    "            \n",
    "        if temperature == 0:\n",
    "            action = (action_probs > 0.5).float()\n",
    "        else:\n",
    "            adjusted_probs = torch.pow(action_probs, 1.0 / temperature)\n",
    "            adjusted_probs = adjusted_probs / (adjusted_probs + torch.pow(1 - action_probs, 1.0 / temperature))\n",
    "            # Sample action from probabilities\n",
    "            dist = Bernoulli(probs=adjusted_probs)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, _, done, _ = client.step({\n",
    "            \"main_thrust\": int(action[0]),\n",
    "            \"left_thrust\": int(action[1]),\n",
    "            \"right_thrust\": int(action[2])\n",
    "        }, game_id)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = calc_reward(next_state)\n",
    "        total_reward += reward['total']\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Update state and step counter\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "    # Return episode statistics\n",
    "    return {\n",
    "        'total_reward': total_reward,\n",
    "        'rewards': rewards,\n",
    "        'steps': steps,\n",
    "        'landed': state.landed,\n",
    "        'crashed': state.crashed,\n",
    "        'final_fuel': state.drone_fuel\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accumulated_rewards(results, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot accumulated rewards for each component over time.\n",
    "    \n",
    "    Args:\n",
    "        results: Output from evaluate_policy() containing 'rewards' list\n",
    "        figsize: Figure size tuple (width, height)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract reward components\n",
    "    reward_dicts = results['rewards']\n",
    "    steps = len(reward_dicts)\n",
    "    \n",
    "    # Get all component keys (exclude 'total')\n",
    "    components = [key for key in reward_dicts[0].keys() if key != 'total']\n",
    "    \n",
    "    # Initialize accumulated rewards\n",
    "    accumulated = {comp: [] for comp in components}\n",
    "    accumulated['total'] = []\n",
    "    \n",
    "    # Calculate accumulated rewards for each component\n",
    "    for comp in components:\n",
    "        cumsum = 0\n",
    "        for reward_dict in reward_dicts:\n",
    "            cumsum += reward_dict[comp]\n",
    "            accumulated[comp].append(cumsum)\n",
    "    \n",
    "    # Calculate accumulated total\n",
    "    cumsum_total = 0\n",
    "    for reward_dict in reward_dicts:\n",
    "        cumsum_total += reward_dict['total']\n",
    "        accumulated['total'].append(cumsum_total)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: All components separately\n",
    "    ax1 = axes[0]\n",
    "    for comp in components:\n",
    "        ax1.plot(accumulated[comp], label=comp, linewidth=2)\n",
    "    \n",
    "    ax1.set_title('Accumulated Reward by Component', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Time Steps')\n",
    "    ax1.set_ylabel('Accumulated Reward')\n",
    "    ax1.legend(loc='best', framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Total accumulated reward\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(accumulated['total'], color='black', linewidth=3, label='Total')\n",
    "    ax2.set_title('Total Accumulated Reward', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Time Steps')\n",
    "    ax2.set_ylabel('Accumulated Reward')\n",
    "    ax2.legend(loc='best', framealpha=0.9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa800a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneGamerBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state)\n",
    "        \n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed426ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DroneGamerBoi()\n",
    "policy.load_state_dict(torch.load('./models/drone_policy_v1.4.pth'))\n",
    "policy.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7323722",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    plot_accumulated_rewards(evaluate_policy(\n",
    "        client, \n",
    "        policy, max_steps=500, temperature=0.2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf580b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
