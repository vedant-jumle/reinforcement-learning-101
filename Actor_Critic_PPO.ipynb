{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6ccdfc",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494734f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from rl_helpers.scalers import *\n",
    "from torch.distributions import Bernoulli\n",
    "from delivery_drone.game.socket_client import DroneGameClient, DroneState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57c8b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab5a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to localhost:5555...\n",
      "Connected to server at localhost:5555\n",
      "Server has 1 game instance(s)\n"
     ]
    }
   ],
   "source": [
    "client = DroneGameClient(port=5555)\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50642911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_helpers.scalers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169f397",
   "metadata": {},
   "source": [
    "### State Variables\n",
    "\n",
    "| Variable | Min | Max | Typical Range | Critical Threshold |\n",
    "|----------|-----|-----|---------------|-------------------|\n",
    "| drone_x | 0 | 1 | [0, 1] | - |\n",
    "| drone_y | 0 | 1 | [0, 1] | - |\n",
    "| drone_vx | -2.5 | 2.5 | [-1.5, 1.5] | - |\n",
    "| drone_vy | -2.5 | 3.0 | [-0.5, 2.5] | - |\n",
    "| drone_angle | -1 | 1 | [-0.5, 0.5] | < 0.111 for landing |\n",
    "| drone_angular_vel | -1.5 | 1.5 | [-0.8, 0.8] | - |\n",
    "| drone_fuel | 0 | 1 | [0, 1] | Episode ends at 0 |\n",
    "| platform_x | 0 | 1 | [0.1, 0.9] | - |\n",
    "| platform_y | 0 | 1 | [0.58, 0.92] | Platform spawns at [350, 550]px |\n",
    "| distance_to_platform | 0 | 1.41 | [0, 1.2] | - |\n",
    "| dx_to_platform | -1.125 | 1.125 | [-1, 1] | < ±0.0625 for landing |\n",
    "| dy_to_platform | -1.083 | 1.083 | [-0.5, 0.8] | - |\n",
    "| speed | 0 | 3.9 | [0, 3.0] | < 0.3 for landing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e72e3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_velocity_alignment(state: DroneState):\n",
    "    \"\"\"\n",
    "    Calculate how well the drone's velocity is aligned with optimal direction to platform.\n",
    "    Returns cosine similarity: 1.0 = perfect alignment, -1.0 = opposite direction\n",
    "    \"\"\"\n",
    "    # Optimal direction: from drone to platform\n",
    "    optimal_dx = state.dx_to_platform\n",
    "    optimal_dy = state.dy_to_platform\n",
    "    optimal_norm = math.sqrt(optimal_dx**2 + optimal_dy**2)\n",
    "\n",
    "    if optimal_norm < 1e-6:  # Already at platform\n",
    "        return 1.0\n",
    "\n",
    "    optimal_dx /= optimal_norm\n",
    "    optimal_dy /= optimal_norm\n",
    "\n",
    "    # Current velocity direction\n",
    "    velocity_norm = state.speed\n",
    "    if velocity_norm < 1e-6:  # Not moving\n",
    "        return 0.0\n",
    "\n",
    "    velocity_dx = state.drone_vx / velocity_norm\n",
    "    velocity_dy = state.drone_vy / velocity_norm\n",
    "\n",
    "    # Cosine similarity\n",
    "    return velocity_dx * optimal_dx + velocity_dy * optimal_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd7e50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity-magnitude-weighted distance reward\n",
    "def calc_reward(state: DroneState, prev_state: DroneState = None):\n",
    "    rewards = {}\n",
    "    total_reward = 0\n",
    "\n",
    "    # Time penalty\n",
    "    minimum_time_penalty = 0\n",
    "    maximum_time_penalty = 1\n",
    "    rewards['time_penalty'] = -inverse_quadratic(\n",
    "        state.distance_to_platform,\n",
    "        decay=100,\n",
    "        scaler=maximum_time_penalty-minimum_time_penalty) - minimum_time_penalty\n",
    "\n",
    "    # Distance-based time penalty\n",
    "    # Penalty gets smaller as drone gets closer to platform\n",
    "    # Uses inverse quadratic function: higher penalty when far, reduces as distance decreases\n",
    "    # Minimum penalty of 0.5, maximum of 2.0 per timestep\n",
    "    total_reward += -0.5#rewards['time_penalty']\n",
    "\n",
    "    velocity_alignment = calc_velocity_alignment(state)\n",
    "    dist = state.distance_to_platform\n",
    "    \n",
    "    rewards['distance'] = 0\n",
    "    rewards['hovering'] = 0\n",
    "\n",
    "    if prev_state is not None:\n",
    "        distance_delta = prev_state.distance_to_platform - state.distance_to_platform\n",
    "        speed = state.speed\n",
    "        \n",
    "        # Calculate velocity toward platform\n",
    "        if dist > 1e-6:\n",
    "            velocity_toward_platform = (\n",
    "                state.drone_vx * state.dx_to_platform +\n",
    "                state.drone_vy * state.dy_to_platform\n",
    "            ) / dist\n",
    "        else:\n",
    "            velocity_toward_platform = 0.0\n",
    "        \n",
    "        MIN_MEANINGFUL_SPEED = 0.15  # Require meaningful velocity\n",
    "        \n",
    "        # Only reward if FAST and ALIGNED and making PROGRESS\n",
    "        if speed >= MIN_MEANINGFUL_SPEED and velocity_toward_platform > 0.1 and dist > 0.065:\n",
    "            # Good: Moving fast toward platform\n",
    "            speed_multiplier = 1.0 + speed * 2.0\n",
    "            rewards['distance'] = float(np.clip(distance_delta * 1000 * speed_multiplier, -2, 5))\n",
    "        elif distance_delta < -0.001:\n",
    "            # BAD: Moving away from platform (distance increasing)\n",
    "            rewards['distance'] = -2.0 * abs(distance_delta) * 1000  # Harsh penalty\n",
    "            rewards['hovering'] = 0  # Don't double-penalize\n",
    "        elif speed < 0.05:\n",
    "            # Hovering\n",
    "            rewards['hovering'] = -1.0\n",
    "        elif speed < MIN_MEANINGFUL_SPEED:\n",
    "            # Too slow\n",
    "            rewards['hovering'] = -0.3\n",
    "        else:\n",
    "            rewards['distance'] = 0.0\n",
    "            \n",
    "    total_reward += rewards['distance']\n",
    "    total_reward += rewards['hovering']\n",
    "\n",
    "    # Angle penalty (define a distance based max threshold)\n",
    "    abs_angle = abs(state.drone_angle)\n",
    "    max_angle = 0.20\n",
    "    max_permissible_angle = ((max_angle-0.111)*dist) + 0.111\n",
    "    excess = abs_angle - max_permissible_angle # excess angle\n",
    "    rewards['angle'] = -max(excess, 0) # maximum reward is 0 (we dont want it to reward hack for stability)\n",
    "\n",
    "    total_reward += rewards['angle']\n",
    "\n",
    "    # Speed - penalize excessive speed\n",
    "    rewards['speed'] = 0\n",
    "    speed = state.speed\n",
    "    max_speed = 0.6\n",
    "    if dist < 1:\n",
    "        rewards['speed'] = -2 * max(speed-0.1, 0)\n",
    "    else:\n",
    "        rewards['speed'] = -1 * max(speed-max_speed, 0)\n",
    "    total_reward += rewards['speed']\n",
    "\n",
    "    # Penalize being below platform\n",
    "    rewards['vertical_position'] = 0\n",
    "    if state.dy_to_platform > 0:  # Platform is below drone (drone is above - GOOD)\n",
    "        rewards['vertical_position'] = 0\n",
    "    else:  # Drone is below platform (BAD!)\n",
    "        rewards['vertical_position'] = state.dy_to_platform * 4.0  # Negative penalty\n",
    "    total_reward += rewards['vertical_position']\n",
    "\n",
    "    # Terminal\n",
    "    rewards['terminal'] = 0\n",
    "    if state.landed:\n",
    "        rewards['terminal'] = 800.0 + state.drone_fuel * 100.0\n",
    "    elif state.crashed:\n",
    "        rewards['terminal'] = -200.0\n",
    "        # Extra penalty for crashing far from target\n",
    "        if state.distance_to_platform > 0.3:\n",
    "            rewards['terminal'] -= 100.0\n",
    "    total_reward += rewards['terminal']\n",
    "\n",
    "    rewards['total'] = total_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c9b983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset()\n",
    "state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a81352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drone_x': 0.86,\n",
       " 'drone_y': 0.2538283333333333,\n",
       " 'drone_vx': 0.0,\n",
       " 'drone_vy': 0.029699999999999997,\n",
       " 'drone_angle': 0.0,\n",
       " 'drone_angular_vel': 0.0,\n",
       " 'drone_fuel': 1.0,\n",
       " 'platform_x': 0.78875,\n",
       " 'platform_y': 0.18833333333333332,\n",
       " 'distance_to_platform': 0.08654166454120524,\n",
       " 'dx_to_platform': -0.07125,\n",
       " 'dy_to_platform': -0.065495,\n",
       " 'speed': 0.029699999999999997,\n",
       " 'landed': False,\n",
       " 'crashed': False,\n",
       " 'steps': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'time_penalty': -0.5717729518600626,\n",
       " 'distance': 0,\n",
       " 'hovering': 0,\n",
       " 'angle': 0,\n",
       " 'speed': 0,\n",
       " 'vertical_position': -0.26198,\n",
       " 'terminal': 0,\n",
       " 'total': -0.76198}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_state = state if state else None\n",
    "client.step(\n",
    "    dict(\n",
    "        main_thrust=0,\n",
    "        left_thrust=0,\n",
    "        right_thrust=0\n",
    "    )\n",
    ")\n",
    "state = client.get_state()\n",
    "display(state.__dict__)\n",
    "calc_reward(state, prev_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a122ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_array(state, device='cpu'):\n",
    "    \"\"\"Convert DroneState dataclass to numpy array\"\"\"\n",
    "    data = np.array([\n",
    "        state.drone_x,\n",
    "        state.drone_y,\n",
    "        state.drone_vx,\n",
    "        state.drone_vy,\n",
    "        state.drone_angle,\n",
    "        state.drone_angular_vel,\n",
    "        state.drone_fuel,\n",
    "        state.platform_x,\n",
    "        state.platform_y,\n",
    "        state.distance_to_platform,\n",
    "        state.dx_to_platform,\n",
    "        state.dy_to_platform,\n",
    "        state.speed,\n",
    "        float(state.landed),\n",
    "        float(state.crashed)\n",
    "    ])\n",
    "    \n",
    "    return torch.tensor(data, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc2f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneGamerBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state, device=device)\n",
    "        \n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1c3fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneTeacherBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) # output will be just a scalar\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state, device=device)\n",
    "        \n",
    "        return (self.network(state)\n",
    "                .squeeze(-1)) # this will convert the tensor from [B, 1] to [B,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abce185",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO): How It Works\n",
    "\n",
    "PPO is an advanced Actor-Critic method that solves a critical problem: **how to update the policy safely without accidentally destroying what it has already learned**.\n",
    "\n",
    "### The Core Problem PPO Solves\n",
    "\n",
    "**In Basic Actor-Critic:**\n",
    "- One bad gradient update can collapse the policy (make it deterministic or terrible)\n",
    "- If we take too large a step, the policy might never recover\n",
    "- No mechanism to prevent catastrophic updates\n",
    "\n",
    "**PPO's solution:** **Clip the policy updates** to stay within a \"trust region\" - only allow small, controlled changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Innovation: The Clipped Objective\n",
    "\n",
    "### 1. Policy Ratio\n",
    "\n",
    "PPO tracks how much the policy has changed by computing a ratio:\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$$\n",
    "\n",
    "**What this ratio means:**\n",
    "- $r_t = 1.0$: Policy unchanged for this state-action pair\n",
    "- $r_t > 1.0$: New policy makes action $a_t$ MORE likely\n",
    "- $r_t < 1.0$: New policy makes action $a_t$ LESS likely\n",
    "\n",
    "### 2. Clipped Surrogate Objective\n",
    "\n",
    "Instead of the standard policy gradient loss, PPO uses:\n",
    "\n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\cdot A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot A_t\\right)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $A_t$ = Advantage (GAE: how good this action was vs average)\n",
    "- $\\epsilon$ = Clipping parameter (typically **0.2**)\n",
    "- $\\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)$ = Constrain ratio to $[0.8, 1.2]$\n",
    "\n",
    "### 3. What the Clipping Does\n",
    "\n",
    "The `min()` creates a **pessimistic bound** - we take the worst-case of the two objectives:\n",
    "\n",
    "**Case 1: Positive Advantage ($A_t > 0$)** - Good action, want to increase probability\n",
    "\n",
    "| Ratio $r_t$ | Effect | Clipping Active? |\n",
    "|-------------|--------|------------------|\n",
    "| $r_t < 0.8$ | Increase probability | No (safe) |\n",
    "| $0.8 \\leq r_t \\leq 1.2$ | Increase probability | No (safe) |\n",
    "| $r_t > 1.2$ | **STOP increasing** | **Yes** (already changed +20%) |\n",
    "\n",
    "**Case 2: Negative Advantage ($A_t < 0$)** - Bad action, want to decrease probability\n",
    "\n",
    "| Ratio $r_t$ | Effect | Clipping Active? |\n",
    "|-------------|--------|------------------|\n",
    "| $r_t < 0.8$ | **STOP decreasing** | **Yes** (already changed -20%) |\n",
    "| $0.8 \\leq r_t \\leq 1.2$ | Decrease probability | No (safe) |\n",
    "| $r_t > 1.2$ | Decrease probability | No (safe) |\n",
    "\n",
    "**The genius:** Clipping prevents the policy from changing too drastically. If an action probability has already changed by ±20%, we stop pushing it further.\n",
    "\n",
    "---\n",
    "\n",
    "## PPO Algorithm Flow\n",
    "\n",
    "### Phase 1: Rollout Collection (Like REINFORCE)\n",
    "\n",
    "```\n",
    "1. Run current policy π_θ_old for N steps across parallel games\n",
    "2. Store: states, actions, log_probs, rewards, dones, values\n",
    "3. Compute GAE advantages for variance reduction\n",
    "```\n",
    "\n",
    "### Phase 2: Multi-Epoch Updates (Key Difference!)\n",
    "\n",
    "Unlike Actor-Critic (1 update per transition), PPO reuses data:\n",
    "\n",
    "```\n",
    "For epoch in 1..K (typically K=4-10):\n",
    "    Shuffle collected data into minibatches\n",
    "    \n",
    "    For each minibatch:\n",
    "        # Compute new policy probabilities\n",
    "        new_log_probs = log π_θ(actions | states)\n",
    "        \n",
    "        # Policy ratio\n",
    "        ratio = exp(new_log_probs - old_log_probs)\n",
    "        \n",
    "        # Clipped objective\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = clip(ratio, 1-ε, 1+ε) * advantages\n",
    "        policy_loss = -min(surr1, surr2).mean()\n",
    "        \n",
    "        # Entropy bonus for exploration\n",
    "        entropy = -sum(probs * log(probs))\n",
    "        policy_loss -= entropy_coef * entropy\n",
    "        \n",
    "        # Update policy\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Value loss (simple MSE)\n",
    "        value_loss = (values - returns)²\n",
    "        critic_optimizer.step()\n",
    "```\n",
    "\n",
    "### Phase 3: Replace Old Policy\n",
    "\n",
    "```\n",
    "θ_old ← θ  # Save current policy for next rollout\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Generalized Advantage Estimation (GAE)\n",
    "\n",
    "PPO typically uses GAE instead of simple TD error for lower variance:\n",
    "\n",
    "### Simple TD Error (Actor-Critic):\n",
    "$$A_t = \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "### GAE (PPO):\n",
    "$$A_t^{\\text{GAE}} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "Where $\\lambda \\in [0, 1]$ controls bias-variance tradeoff:\n",
    "- $\\lambda = 0$: Pure TD (low variance, high bias)\n",
    "- $\\lambda = 1$: Monte Carlo (high variance, low bias)  \n",
    "- $\\lambda = 0.95$: **PPO sweet spot** ✓\n",
    "\n",
    "**Practical computation (backward pass):**\n",
    "```python\n",
    "advantages = []\n",
    "gae = 0\n",
    "for t in reversed(range(T)):\n",
    "    delta = rewards[t] + gamma * values[t+1] - values[t]\n",
    "    gae = delta + gamma * lambda_ * gae\n",
    "    advantages.insert(0, gae)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## PPO vs Actor-Critic vs REINFORCE\n",
    "\n",
    "| Feature | REINFORCE | Actor-Critic | **PPO** |\n",
    "|---------|-----------|--------------|---------|\n",
    "| **Update frequency** | End of episode | Every step | Every N steps |\n",
    "| **Data reuse** | No | No | **Yes (K epochs)** ✓ |\n",
    "| **Variance** | Very high | Medium | **Low (GAE)** ✓ |\n",
    "| **Stability** | Unstable | Can be unstable | **Very stable (clipping)** ✓ |\n",
    "| **Sample efficiency** | Poor | Good | **Best** ✓ |\n",
    "| **Prevents collapse** | No | No | **Yes (trust region)** ✓ |\n",
    "\n",
    "---\n",
    "\n",
    "## Why PPO Wins\n",
    "\n",
    "### 1. **Prevents Policy Collapse**\n",
    "\n",
    "**Without clipping:**\n",
    "```python\n",
    "# One huge gradient can ruin the policy\n",
    "loss = -log_prob * advantage  # No bounds!\n",
    "# If advantage is +100, gradient is HUGE → policy becomes deterministic\n",
    "```\n",
    "\n",
    "**With clipping:**\n",
    "```python\n",
    "ratio = new_prob / old_prob\n",
    "clipped_ratio = clip(ratio, 0.8, 1.2)\n",
    "loss = -min(ratio * advantage, clipped_ratio * advantage)\n",
    "# Even if advantage is +100, change is limited to ±20%\n",
    "```\n",
    "\n",
    "### 2. **Data Efficiency**\n",
    "\n",
    "- **Actor-Critic**: Use each transition once → throw away\n",
    "- **PPO**: Reuse data 4-10 times → fewer environment interactions\n",
    "\n",
    "### 3. **Stable Across Tasks**\n",
    "\n",
    "- Works out-of-the-box on many tasks\n",
    "- Requires minimal hyperparameter tuning\n",
    "- Industry standard (OpenAI Five, AlphaStar, robotics)\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Intuition: Why min()?\n",
    "\n",
    "```python\n",
    "L = -min(ratio * A, clip(ratio, 1-ε, 1+ε) * A)\n",
    "     ↑\n",
    "   Pessimistic: take minimum (worst-case) of two terms\n",
    "```\n",
    "\n",
    "**If $A_t > 0$ (good action):**\n",
    "- Unclipped: $r_t \\cdot A$ encourages increasing probability\n",
    "- Clipped: Caps increase once $r_t > 1.2$\n",
    "- **min() chooses:** Once ratio exceeds 1.2, gradient becomes zero (stops increasing)\n",
    "\n",
    "**If $A_t < 0$ (bad action):**\n",
    "- Unclipped: $r_t \\cdot A$ (negative) encourages decreasing probability\n",
    "- Clipped: Caps decrease once $r_t < 0.8$  \n",
    "- **min() chooses more negative:** Once ratio drops below 0.8, gradient becomes zero (stops decreasing)\n",
    "\n",
    "The `min()` is **conservative** - it says: \"Don't be overconfident about policy improvements.\"\n",
    "\n",
    "---\n",
    "\n",
    "## PPO Hyperparameters\n",
    "\n",
    "```python\n",
    "# Policy\n",
    "learning_rate_actor = 3e-4       # Standard PPO LR\n",
    "clip_epsilon = 0.2               # Trust region (±20%)\n",
    "\n",
    "# Value function\n",
    "learning_rate_critic = 1e-3      # Often higher than actor\n",
    "value_loss_coef = 0.5            # Weight of critic loss\n",
    "\n",
    "# Training\n",
    "num_epochs = 10                  # Reuse data this many times\n",
    "minibatch_size = 64              # For SGD updates\n",
    "rollout_steps = 2048             # Steps to collect before update\n",
    "\n",
    "# Advantage\n",
    "gamma = 0.99                     # Discount factor\n",
    "gae_lambda = 0.95                # GAE parameter\n",
    "\n",
    "# Regularization\n",
    "entropy_coef = 0.01              # Exploration bonus\n",
    "max_grad_norm = 0.5              # Gradient clipping\n",
    "```\n",
    "\n",
    "### Key Effects\n",
    "\n",
    "| Parameter | Too Low | Too High | Sweet Spot |\n",
    "|-----------|---------|----------|------------|\n",
    "| `clip_epsilon` | Policy barely changes | Allows catastrophic updates | **0.2** |\n",
    "| `num_epochs` | Wastes data | Overfits to old data | **4-10** |\n",
    "| `gae_lambda` | High bias | High variance | **0.95** |\n",
    "| `entropy_coef` | No exploration | Too random | **0.01** |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use PPO\n",
    "\n",
    "### ✅ Use PPO when:\n",
    "- You want **stable, reliable training** (most important!)\n",
    "- You can collect batches of data before updating\n",
    "- Working on **continuous control** (robotics, drone landing)\n",
    "- Want **sample efficiency** (fewer env interactions)\n",
    "- Need algo that **works out-of-the-box**\n",
    "\n",
    "### ❌ Use Actor-Critic when:\n",
    "- Need **immediate online learning** (can't wait for batches)\n",
    "- Environment is cheap to simulate (sample efficiency less critical)\n",
    "- Prefer simplicity over performance\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**PPO Formula:**\n",
    "> Trust regions through clipping - update safely!\n",
    "\n",
    "**Three Keys:**\n",
    "1. **Clipped objective:** Bound policy changes to ±20%\n",
    "2. **Multiple epochs:** Reuse data 4-10 times  \n",
    "3. **GAE advantages:** Low-variance advantage estimates\n",
    "\n",
    "**Why it's the industry default:**\n",
    "- Stable (no policy collapse)\n",
    "- Sample efficient (reuses data)\n",
    "- Robust (works across many tasks)\n",
    "\n",
    "For **drone landing**: PPO will likely outperform basic Actor-Critic due to superior stability and data efficiency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab2259bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "y7md3o61f4l",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95, device=None):\n",
    "    \"\"\"\n",
    "    Compute GAE advantages using torch. Accepts python lists or torch tensors.\n",
    "\n",
    "    Args:\n",
    "        rewards: sequence (list or tensor) length T of rewards (r_0 ... r_{T-1})\n",
    "        values: sequence (list or tensor) of value estimates. Preferably length T+1 (V_0 ... V_T).\n",
    "                If only T values are provided, a 0 bootstrap is appended.\n",
    "        dones:   sequence (list or tensor) length T of done flags (0/1 or booleans) or a scalar (broadcast).\n",
    "        gamma:   discount factor\n",
    "        lambda_: GAE lambda\n",
    "        device:  torch device to place tensors on (defaults to values.device or cpu)\n",
    "    Returns:\n",
    "        advantages: torch tensor shape (T,) dtype float32 on chosen device\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    rewards = torch.as_tensor(rewards, dtype=torch.float32)\n",
    "    values = torch.as_tensor(values, dtype=torch.float32)\n",
    "    dones = torch.as_tensor(dones, dtype=torch.float32)\n",
    "\n",
    "    # determine device\n",
    "    if device is None:\n",
    "        device = values.device if values.numel() > 0 else torch.device('cpu')\n",
    "    rewards = rewards.to(device)\n",
    "    values = values.to(device)\n",
    "    dones = dones.to(device)\n",
    "\n",
    "    T = rewards.shape[0]\n",
    "\n",
    "    # flatten value shape\n",
    "    if values.dim() > 1 and values.shape[-1] == 1:\n",
    "        values = values.squeeze(-1)\n",
    "\n",
    "    # ensure values has length T+1 (bootstrap). If only T provided, append 0.\n",
    "    if values.shape[0] == T:\n",
    "        values = torch.cat([values, torch.zeros(1, device=device, dtype=values.dtype)], dim=0)\n",
    "    elif values.shape[0] < T + 1:\n",
    "        raise ValueError(f\"values must have length >= T (got {values.shape[0]} vs T+1={T+1})\")\n",
    "\n",
    "    # handle dones: if scalar or length 1, broadcast to T\n",
    "    if dones.numel() == 1:\n",
    "        dones = dones.expand(T)\n",
    "    if dones.shape[0] != T:\n",
    "        raise ValueError(f\"dones must have length T (got {dones.shape[0]} vs T={T})\")\n",
    "\n",
    "    advantages = torch.zeros(T, dtype=torch.float32, device=device)\n",
    "    gae = torch.tensor(0.0, dtype=torch.float32, device=device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        mask = 1.0 - dones[t]  # 0 if done, 1 otherwise\n",
    "        delta = rewards[t] + gamma * values[t + 1] * mask - values[t]\n",
    "        gae = delta + gamma * lambda_ * mask * gae\n",
    "        advantages[t] = gae\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "xpbyq3cuafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes_ppo(client: DroneGameClient, policy: DroneGamerBoi, max_steps=300):\n",
    "    \"\"\"\n",
    "    Collect complete episodes for PPO training.\n",
    "    \n",
    "    KEY DIFFERENCE FROM GAE: Stores log_probs during collection for ratio computation.\n",
    "    \n",
    "    Args:\n",
    "        client: DroneGameClient instance\n",
    "        policy: Policy network\n",
    "        max_steps: Maximum steps per episode (default: 300)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (states, actions, log_probs, rewards, done, final_state)\n",
    "        - states: List of state tensors\n",
    "        - actions: List of action tensors\n",
    "        - log_probs: List of log probability tensors (OLD policy)\n",
    "        - rewards: List of reward scalars\n",
    "        - done: Bool indicating if episode ended naturally (landed/crashed)\n",
    "        - final_state: Final DroneState for bootstrap value computation\n",
    "    \"\"\"\n",
    "    num_games = client.num_games\n",
    "    \n",
    "    # Initialize storage\n",
    "    all_episodes = [\n",
    "        {\n",
    "            'states': [], \n",
    "            'actions': [], \n",
    "            'log_probs': [],  # NEW FOR PPO: store old policy log probs\n",
    "            'rewards': [],\n",
    "            'prev_states': [],  # For velocity-weighted rewards\n",
    "            'done': False,\n",
    "            'final_state': None\n",
    "        } \n",
    "        for _ in range(num_games)\n",
    "    ]\n",
    "    \n",
    "    # Reset all games\n",
    "    game_states = [client.reset(game_id) for game_id in range(num_games)]\n",
    "    prev_game_states = [None] * num_games  # Track previous states for rewards\n",
    "    step_counts = [0] * num_games\n",
    "    \n",
    "    while not all(ep['done'] for ep in all_episodes):\n",
    "        # Batch active games\n",
    "        batch_states = []\n",
    "        active_game_ids = []\n",
    "        \n",
    "        for game_id in range(num_games):\n",
    "            if not all_episodes[game_id]['done']:\n",
    "                batch_states.append(state_to_array(game_states[game_id], device=device))\n",
    "                active_game_ids.append(game_id)\n",
    "        \n",
    "        if len(batch_states) == 0:\n",
    "            break\n",
    "        \n",
    "        # Batched inference\n",
    "        batch_states_tensor = torch.stack(batch_states)\n",
    "        \n",
    "        with torch.no_grad():  # Don't need gradients during collection\n",
    "            batch_action_probs = policy(batch_states_tensor)\n",
    "        \n",
    "        batch_dist = Bernoulli(probs=batch_action_probs)\n",
    "        batch_actions = batch_dist.sample()\n",
    "        batch_log_probs = batch_dist.log_prob(batch_actions).sum(dim=1)  # Store for PPO\n",
    "        \n",
    "        # Execute actions for each active game\n",
    "        for i, game_id in enumerate(active_game_ids):\n",
    "            action = batch_actions[i]\n",
    "            log_prob = batch_log_probs[i]  # OLD policy log prob\n",
    "            state_tensor = batch_states[i]\n",
    "            \n",
    "            # Store previous state for reward calculation\n",
    "            prev_state = prev_game_states[game_id]\n",
    "            current_state = game_states[game_id]\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, _, done, _ = client.step({\n",
    "                \"main_thrust\": int(action[0]),\n",
    "                \"left_thrust\": int(action[1]),\n",
    "                \"right_thrust\": int(action[2])\n",
    "            }, game_id)\n",
    "            \n",
    "            # Compute reward WITH prev_state for velocity-magnitude weighting\n",
    "            reward_dict = calc_reward(next_state, prev_state=prev_state)\n",
    "            reward = reward_dict['total']\n",
    "            \n",
    "            # Update step count\n",
    "            step_counts[game_id] += 1\n",
    "            \n",
    "            # Check for timeout\n",
    "            if step_counts[game_id] >= max_steps:\n",
    "                if not next_state.landed:\n",
    "                    reward -= 500  # Timeout penalty\n",
    "                done = True  # Force termination\n",
    "            \n",
    "            # Store transition data INCLUDING log_probs for PPO\n",
    "            all_episodes[game_id]['states'].append(state_tensor)\n",
    "            all_episodes[game_id]['actions'].append(action)\n",
    "            all_episodes[game_id]['log_probs'].append(log_prob)  # Store old policy log prob\n",
    "            all_episodes[game_id]['rewards'].append(reward)\n",
    "            \n",
    "            # Update state tracking\n",
    "            prev_game_states[game_id] = current_state\n",
    "            game_states[game_id] = next_state\n",
    "            \n",
    "            # Check if episode finished\n",
    "            if done:\n",
    "                all_episodes[game_id]['done'] = done  # True if landed/crashed, False if timeout\n",
    "                all_episodes[game_id]['final_state'] = next_state\n",
    "    \n",
    "    # Return episodes with log_probs for PPO\n",
    "    return [\n",
    "        (\n",
    "            ep['states'],           # List of state tensors\n",
    "            ep['actions'],          # List of action tensors  \n",
    "            ep['log_probs'],        # List of log prob tensors (OLD policy)\n",
    "            ep['rewards'],          # List of reward scalars\n",
    "            ep['done'],             # Bool: True if natural end, False if timeout\n",
    "            ep['final_state']       # Final DroneState for bootstrap\n",
    "        )\n",
    "        for ep in all_episodes\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "qfw80yaoyxq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppo_losses(\n",
    "    policy, critic, states, actions, old_log_probs, advantages, returns,\n",
    "    clip_epsilon=0.2, entropy_coef=0.01, value_coef=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute PPO clipped loss and value loss.\n",
    "\n",
    "    Args:\n",
    "        policy: Actor network\n",
    "        critic: Critic network\n",
    "        states: State tensors\n",
    "        actions: Action tensors\n",
    "        old_log_probs: Log probs from old policy (stored during collection)\n",
    "        advantages: GAE advantages (detached)\n",
    "        returns: Advantage + value estimates (detached)\n",
    "        clip_epsilon: Clipping range for ratio (typically 0.2)\n",
    "        entropy_coef: Entropy bonus coefficient\n",
    "        value_coef: Value loss coefficient\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: total_loss, policy_loss, value_loss, entropy, ratio_stats\n",
    "    \"\"\"\n",
    "    # 1. Compute new log probabilities (policy has been updated)\n",
    "    action_probs = policy(states)\n",
    "    action_probs = torch.clamp(action_probs, 1e-8, 1 - 1e-8)  # Prevent NaN\n",
    "    dist = Bernoulli(probs=action_probs)\n",
    "    new_log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    # 2. Compute ratio: π_new / π_old = exp(log π_new - log π_old)\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "    # 3. Clipped surrogate objective\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    # 4. Value loss (MSE)\n",
    "    values = critic(states)\n",
    "    value_loss = ((values - returns) ** 2).mean()\n",
    "\n",
    "    # 5. Total loss with entropy bonus\n",
    "    total_loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "    # 6. Compute clipping statistics (for monitoring)\n",
    "    with torch.no_grad():\n",
    "        ratio_stats = {\n",
    "            'mean': ratio.mean().item(),\n",
    "            'min': ratio.min().item(),\n",
    "            'max': ratio.max().item(),\n",
    "            'std': ratio.std().item(),\n",
    "            'clipped_frac': ((ratio < (1 - clip_epsilon)) | (ratio > (1 + clip_epsilon))).float().mean().item()\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'policy_loss': policy_loss,\n",
    "        'value_loss': value_loss,\n",
    "        'entropy': entropy,\n",
    "        'ratio_stats': ratio_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "725a6md3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_simple(client, policy, max_steps=300, game_id=0, temperature=0.5, iteration=0, fig_ax=None):\n",
    "    \"\"\"\n",
    "    Simple evaluation with static plots that reuse the same figure.\n",
    "    Auto-detects reward components from calc_reward() output.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    policy.eval()\n",
    "    state = client.reset(game_id)\n",
    "    prev_state = None\n",
    "    \n",
    "    # Initialize with empty dicts - will be populated dynamically\n",
    "    history = {'step': []}\n",
    "    accumulated = {}\n",
    "    \n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Run episode\n",
    "    while not done and steps < max_steps:\n",
    "        with torch.no_grad():\n",
    "            action_probs = policy(state)\n",
    "            \n",
    "        if temperature == 0:\n",
    "            action = (action_probs > 0.5).float()\n",
    "        else:\n",
    "            adjusted_probs = torch.pow(action_probs, 1.0 / temperature)\n",
    "            adjusted_probs = adjusted_probs / (adjusted_probs + torch.pow(1 - action_probs, 1.0 / temperature))\n",
    "            dist = Bernoulli(probs=adjusted_probs)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        prev_state = state\n",
    "        next_state, _, done, _ = client.step({\n",
    "            \"main_thrust\": int(action[0]),\n",
    "            \"left_thrust\": int(action[1]),\n",
    "            \"right_thrust\": int(action[2])\n",
    "        }, game_id)\n",
    "        \n",
    "        reward = calc_reward(next_state, prev_state=prev_state)\n",
    "        \n",
    "        # Initialize components on first step\n",
    "        if steps == 0:\n",
    "            for key in reward.keys():\n",
    "                history[key] = []\n",
    "                accumulated[key] = 0\n",
    "        \n",
    "        # Accumulate rewards\n",
    "        for key in reward.keys():\n",
    "            accumulated[key] += reward[key]\n",
    "        \n",
    "        # Store history\n",
    "        history['step'].append(steps)\n",
    "        for key in reward.keys():\n",
    "            history[key].append(accumulated[key])\n",
    "        \n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    # Create or reuse figure\n",
    "    if fig_ax is None:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    else:\n",
    "        fig, (ax1, ax2) = fig_ax\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "    \n",
    "    # Plot 1: All components (auto-detected, exclude 'total')\n",
    "    components = [key for key in history.keys() if key not in ['step', 'total']]\n",
    "    for comp in components:\n",
    "        ax1.plot(history['step'], history[comp], label=comp, linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Time Steps', fontsize=11)\n",
    "    ax1.set_ylabel('Accumulated Reward', fontsize=11)\n",
    "    ax1.set_title(f'Accumulated Reward by Component (Iter {iteration})', fontweight='bold', fontsize=12)\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Total\n",
    "    if 'total' in history:\n",
    "        ax2.plot(history['step'], history['total'], color='black', linewidth=3)\n",
    "    ax2.set_xlabel('Time Steps', fontsize=11)\n",
    "    ax2.set_ylabel('Accumulated Reward', fontsize=11)\n",
    "    ax2.set_title(f'Total Accumulated Reward (Iter {iteration})', fontweight='bold', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add result annotation\n",
    "    status = \"[LANDED]\" if state.landed else \"[CRASHED]\"\n",
    "    color = 'green' if state.landed else 'red'\n",
    "    result_text = f\"{status} | Steps: {steps} | Total: {accumulated['total']:.1f} | Fuel: {state.drone_fuel:.1%}\"\n",
    "    \n",
    "    # Clear previous suptitle and add new one\n",
    "    fig.suptitle(result_text, fontsize=13, fontweight='bold', color=color, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the figure using IPython display\n",
    "    display(fig)\n",
    "    \n",
    "    return {\n",
    "        'landed': state.landed,\n",
    "        'steps': steps,\n",
    "        'total_reward': accumulated['total'],\n",
    "        'final_fuel': state.drone_fuel\n",
    "    }, (fig, (ax1, ax2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ua2403g9yh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Hyperparameters:\n",
      "  Iterations: 2000\n",
      "  Epochs per iteration: 4\n",
      "  Clip epsilon: 0.2\n",
      "  Entropy coefficient: 0.01\n",
      "  Value coefficient: 0.5\n",
      "  GAE lambda: 0.95\n",
      "  Gamma: 0.99\n",
      "  Episode length: 300 → 500\n"
     ]
    }
   ],
   "source": [
    "# PPO Hyperparameters\n",
    "num_iterations = 2000       # More iterations for PPO\n",
    "num_epochs = 4              # Number of times to reuse collected data\n",
    "clip_epsilon = 0.2          # Clipping range for ratio\n",
    "entropy_coef = 0.01         # Entropy bonus coefficient\n",
    "value_coef = 0.5            # Value loss coefficient\n",
    "gae_lambda = 0.95           # GAE parameter\n",
    "gamma = 0.99                # Discount factor\n",
    "max_grad_norm = 0.5         # Gradient clipping\n",
    "eval_interval = 10          # Evaluate every N iterations\n",
    "\n",
    "# Episode schedule (curriculum learning)\n",
    "steepness = 0.65\n",
    "start = 300\n",
    "end = 500\n",
    "x = np.linspace(0, 1, num=num_iterations)\n",
    "step_schedule = np.round(start + (end - start) * x**steepness).astype(np.int32)\n",
    "\n",
    "print(\"PPO Hyperparameters:\")\n",
    "print(f\"  Iterations: {num_iterations}\")\n",
    "print(f\"  Epochs per iteration: {num_epochs}\")\n",
    "print(f\"  Clip epsilon: {clip_epsilon}\")\n",
    "print(f\"  Entropy coefficient: {entropy_coef}\")\n",
    "print(f\"  Value coefficient: {value_coef}\")\n",
    "print(f\"  GAE lambda: {gae_lambda}\")\n",
    "print(f\"  Gamma: {gamma}\")\n",
    "print(f\"  Episode length: {start} → {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cl0px7eplqp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks for PPO training\n",
    "policy = DroneGamerBoi().to(device)\n",
    "critic = DroneTeacherBoi().to(device)\n",
    "\n",
    "# PPO typically uses lower learning rates\n",
    "policy_optimizer = torch.optim.AdamW(policy.parameters(), lr=3e-4)\n",
    "critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Networks initialized for PPO training!\")\n",
    "print(f\"Policy learning rate: 3e-4\")\n",
    "print(f\"Critic learning rate: 1e-3\")\n",
    "print(f\"Policy device: {next(policy.parameters()).device}\")\n",
    "print(f\"Critic device: {next(critic.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6wby0b62jbn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Training Loop with Metrics Tracking\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "num_games = client.num_games\n",
    "eval_fig_ax = None\n",
    "\n",
    "# Initialize metrics storage\n",
    "training_metrics = {\n",
    "    'hyperparameters': {\n",
    "        'num_iterations': num_iterations,\n",
    "        'num_epochs': num_epochs,\n",
    "        'clip_epsilon': clip_epsilon,\n",
    "        'entropy_coef': entropy_coef,\n",
    "        'value_coef': value_coef,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'gamma': gamma,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'policy_lr': 3e-4,\n",
    "        'critic_lr': 1e-3,\n",
    "        'num_parallel_games': num_games\n",
    "    },\n",
    "    'iterations': []\n",
    "}\n",
    "\n",
    "tqdm_iterations = trange(num_iterations, desc='', total=num_iterations)\n",
    "\n",
    "for iteration in tqdm_iterations:\n",
    "    max_steps = step_schedule[iteration]\n",
    "    \n",
    "    # PHASE 1: Collect episodes ONCE (with old policy)\n",
    "    episodes = collect_episodes_ppo(client, policy, max_steps=int(max_steps))\n",
    "    \n",
    "    episode_lengths = []\n",
    "    episode_rewards = []\n",
    "    num_successes = 0\n",
    "    \n",
    "    # PHASE 2: Compute advantages using GAE (ONCE per iteration)\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    all_old_log_probs = []\n",
    "    all_advantages = []\n",
    "    all_returns = []\n",
    "\n",
    "    for (states, actions, log_probs, rewards, done, final_state) in episodes:\n",
    "        episode_lengths.append(len(states))\n",
    "        episode_rewards.append(sum(rewards))\n",
    "\n",
    "        # Stack into tensors\n",
    "        states_tensor = torch.stack(states)\n",
    "        actions_tensor = torch.stack(actions)\n",
    "        old_log_probs_tensor = torch.stack(log_probs)  # OLD policy log probs\n",
    "\n",
    "        # Compute values for GAE\n",
    "        with torch.no_grad():\n",
    "            values = critic(states_tensor)\n",
    "\n",
    "        # Bootstrap value\n",
    "        if done:\n",
    "            bootstrap_value = 0.0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                bootstrap_value = critic(state_to_array(final_state, device=device)).item()\n",
    "\n",
    "        values_with_bootstrap = torch.cat([\n",
    "            values,\n",
    "            torch.tensor([bootstrap_value], device=device)\n",
    "        ])\n",
    "\n",
    "        # Compute GAE advantages\n",
    "        dones = [False] * len(rewards)\n",
    "        if done:\n",
    "            dones[-1] = True\n",
    "\n",
    "        advantages = compute_gae(\n",
    "            rewards,\n",
    "            values_with_bootstrap.cpu().numpy(),\n",
    "            dones,\n",
    "            gamma=gamma,\n",
    "            lambda_=gae_lambda,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        returns = advantages + values  # G = A + V\n",
    "\n",
    "        # Store for training\n",
    "        all_states.append(states_tensor)\n",
    "        all_actions.append(actions_tensor)\n",
    "        all_old_log_probs.append(old_log_probs_tensor)\n",
    "        all_advantages.append(advantages)\n",
    "        all_returns.append(returns)\n",
    "\n",
    "        # Track success\n",
    "        if done and final_state.landed:\n",
    "            num_successes += 1\n",
    "\n",
    "    # Concatenate all episodes\n",
    "    all_states = torch.cat(all_states)\n",
    "    all_actions = torch.cat(all_actions)\n",
    "    all_old_log_probs = torch.cat(all_old_log_probs)\n",
    "    all_advantages = torch.cat(all_advantages)\n",
    "    all_returns = torch.cat(all_returns)\n",
    "\n",
    "    # Normalize advantages across ALL episodes\n",
    "    all_advantages = (all_advantages - all_advantages.mean()) / (all_advantages.std() + 1e-8)\n",
    "\n",
    "    # PHASE 3: Multi-epoch updates (KEY DIFFERENCE FOR PPO)\n",
    "    epoch_policy_losses = []\n",
    "    epoch_value_losses = []\n",
    "    epoch_entropies = []\n",
    "    epoch_ratio_means = []\n",
    "    epoch_ratio_mins = []\n",
    "    epoch_ratio_maxs = []\n",
    "    epoch_ratio_stds = []\n",
    "    epoch_clipped_fracs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # PPO: Update networks K times on the same data\n",
    "        losses_dict = compute_ppo_losses(\n",
    "            policy, critic,\n",
    "            all_states,\n",
    "            all_actions,\n",
    "            all_old_log_probs,\n",
    "            all_advantages.detach(),\n",
    "            all_returns.detach(),\n",
    "            clip_epsilon=clip_epsilon,\n",
    "            entropy_coef=entropy_coef,\n",
    "            value_coef=value_coef\n",
    "        )\n",
    "\n",
    "        # Extract losses and stats\n",
    "        total_loss = losses_dict['total_loss']\n",
    "        policy_loss = losses_dict['policy_loss']\n",
    "        value_loss = losses_dict['value_loss']\n",
    "        entropy = losses_dict['entropy']\n",
    "        ratio_stats = losses_dict['ratio_stats']\n",
    "\n",
    "        # Combined optimizer step\n",
    "        policy_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=max_grad_norm)\n",
    "        policy_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        epoch_policy_losses.append(policy_loss.item())\n",
    "        epoch_value_losses.append(value_loss.item())\n",
    "        epoch_entropies.append(entropy.item())\n",
    "        epoch_ratio_means.append(ratio_stats['mean'])\n",
    "        epoch_ratio_mins.append(ratio_stats['min'])\n",
    "        epoch_ratio_maxs.append(ratio_stats['max'])\n",
    "        epoch_ratio_stds.append(ratio_stats['std'])\n",
    "        epoch_clipped_fracs.append(ratio_stats['clipped_frac'])\n",
    "\n",
    "    # Progress tracking\n",
    "    avg_reward = sum(episode_rewards) / num_games\n",
    "    avg_steps = sum(episode_lengths) / num_games\n",
    "    avg_policy_loss = sum(epoch_policy_losses) / num_epochs\n",
    "    avg_value_loss = sum(epoch_value_losses) / num_epochs\n",
    "    avg_entropy = sum(epoch_entropies) / num_epochs\n",
    "    avg_ratio = sum(epoch_ratio_means) / num_epochs\n",
    "    avg_clipped_frac = sum(epoch_clipped_fracs) / num_epochs\n",
    "\n",
    "    # Store iteration metrics\n",
    "    iteration_data = {\n",
    "        'iteration': iteration,\n",
    "        'success_rate': num_successes / num_games,\n",
    "        'num_successes': num_successes,\n",
    "        'avg_reward': avg_reward,\n",
    "        'avg_steps': avg_steps,\n",
    "        'max_steps': int(max_steps),\n",
    "        'policy_loss': avg_policy_loss,\n",
    "        'value_loss': avg_value_loss,\n",
    "        'entropy': avg_entropy,\n",
    "        'ratio_mean': avg_ratio,\n",
    "        'ratio_min': min(epoch_ratio_mins),\n",
    "        'ratio_max': max(epoch_ratio_maxs),\n",
    "        'ratio_std': sum(epoch_ratio_stds) / num_epochs,\n",
    "        'clipped_frac': avg_clipped_frac,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths\n",
    "    }\n",
    "    training_metrics['iterations'].append(iteration_data)\n",
    "\n",
    "    tqdm_iterations.set_description(\n",
    "        f'Success: {num_successes}/{num_games} | '\n",
    "        f'Reward: {avg_reward:.1f} | '\n",
    "        f'Policy: {avg_policy_loss:.4f} | '\n",
    "        f'Value: {avg_value_loss:.4f} | '\n",
    "        f'Ratio: {avg_ratio:.3f} | '\n",
    "        f'Clip%: {avg_clipped_frac:.1%} | '\n",
    "        f'Steps: {avg_steps:.1f}'\n",
    "    )\n",
    "\n",
    "    # Periodic evaluation and saving\n",
    "    if (iteration + 1) % eval_interval == 0:\n",
    "        eval_result, eval_fig_ax = evaluate_policy_simple(\n",
    "            client, \n",
    "            policy, \n",
    "            max_steps=500,\n",
    "            temperature=0.3,\n",
    "            iteration=iteration + 1,\n",
    "            game_id=0,\n",
    "            fig_ax=eval_fig_ax\n",
    "        )\n",
    "        \n",
    "        # Save models\n",
    "        os.makedirs('./models/actor-critic-ppo', exist_ok=True)\n",
    "        torch.save(policy.state_dict(), './models/actor-critic-ppo/drone_policy_v1.pth')\n",
    "        torch.save(critic.state_dict(), './models/actor-critic-ppo/drone_critic_v1.pth')\n",
    "        \n",
    "        # Save metrics every 100 iterations\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            metrics_path = f'./models/actor-critic-ppo/training_metrics_iter{iteration+1}.json'\n",
    "            with open(metrics_path, 'w') as f:\n",
    "                json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "# Final save of complete metrics\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "final_metrics_path = f'./models/actor-critic-ppo/training_metrics_final_{timestamp}.json'\n",
    "with open(final_metrics_path, 'w') as f:\n",
    "    json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining complete! Metrics saved to: {final_metrics_path}\")\n",
    "print(f\"Total iterations: {num_iterations}\")\n",
    "print(f\"Final success rate: {training_metrics['iterations'][-1]['success_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9nn800dexu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "import os\n",
    "os.makedirs('./models/actor-critic-ppo', exist_ok=True)\n",
    "\n",
    "torch.save(policy.state_dict(), './models/actor-critic-ppo/drone_policy_v1.pth')\n",
    "torch.save(critic.state_dict(), './models/actor-critic-ppo/drone_critic_v1.pth')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "print(\"  Policy: ./models/actor-critic-ppo/drone_policy_v1.pth\")\n",
    "print(\"  Critic: ./models/actor-critic-ppo/drone_critic_v1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6019fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "policy = DroneGamerBoi().to('cuda')\n",
    "critic = DroneTeacherBoi().to('cuda')\n",
    "\n",
    "policy.load_state_dict(torch.load('./models/actor-critic-ppo/drone_policy_v1.pth'))\n",
    "critic.load_state_dict(torch.load('./models/actor-critic-ppo/drone_critic_v1.pth'))\n",
    "\n",
    "policy.eval()\n",
    "critic.eval()\n",
    "\n",
    "print(\"Models loaded successfully!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c28a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(client, policy, max_steps=300, game_id=0, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate policy on a single game without training.\n",
    "    \n",
    "    Args:\n",
    "        client (DroneGameClient): Game client instance\n",
    "        policy (DroneGamerBoi): Policy network\n",
    "        max_steps (int): Maximum steps per episode\n",
    "        game_id (int): ID of the game to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Episode statistics including rewards, steps, and outcome\n",
    "    \"\"\"\n",
    "    # Set policy to evaluation mode\n",
    "    policy.eval()\n",
    "    \n",
    "    # Initialize episode\n",
    "    state = client.reset(game_id)\n",
    "    prev_state = None\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Run episode\n",
    "    while not done and steps < max_steps:\n",
    "        # Get action probabilities from policy\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            action_probs = policy(state)\n",
    "            \n",
    "        if temperature == 0:\n",
    "            action = (action_probs > 0.5).float()\n",
    "        else:\n",
    "            adjusted_probs = torch.pow(action_probs, 1.0 / temperature)\n",
    "            adjusted_probs = adjusted_probs / (adjusted_probs + torch.pow(1 - action_probs, 1.0 / temperature))\n",
    "            # Sample action from probabilities\n",
    "            dist = Bernoulli(probs=adjusted_probs)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Take action in environment\n",
    "        prev_state = state\n",
    "        next_state, _, done, _ = client.step({\n",
    "            \"main_thrust\": int(action[0]),\n",
    "            \"left_thrust\": int(action[1]),\n",
    "            \"right_thrust\": int(action[2])\n",
    "        }, game_id)\n",
    "        \n",
    "        # Calculate reward with prev_state\n",
    "        reward = calc_reward(next_state, prev_state=prev_state)\n",
    "        total_reward += reward['total']\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Update state and step counter\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "    # Return episode statistics\n",
    "    return {\n",
    "        'total_reward': total_reward,\n",
    "        'rewards': rewards,\n",
    "        'steps': steps,\n",
    "        'landed': state.landed,\n",
    "        'crashed': state.crashed,\n",
    "        'final_fuel': state.drone_fuel\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65976acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accumulated_rewards(results, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot accumulated rewards for each component over time.\n",
    "    \n",
    "    Args:\n",
    "        results: Output from evaluate_policy() containing 'rewards' list\n",
    "        figsize: Figure size tuple (width, height)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract reward components\n",
    "    reward_dicts = results['rewards']\n",
    "    steps = len(reward_dicts)\n",
    "    \n",
    "    # Get all component keys (exclude 'total')\n",
    "    components = [key for key in reward_dicts[0].keys() if key != 'total']\n",
    "    \n",
    "    # Initialize accumulated rewards\n",
    "    accumulated = {comp: [] for comp in components}\n",
    "    accumulated['total'] = []\n",
    "    \n",
    "    # Calculate accumulated rewards for each component\n",
    "    for comp in components:\n",
    "        cumsum = 0\n",
    "        for reward_dict in reward_dicts:\n",
    "            cumsum += reward_dict[comp]\n",
    "            accumulated[comp].append(cumsum)\n",
    "    \n",
    "    # Calculate accumulated total\n",
    "    cumsum_total = 0\n",
    "    for reward_dict in reward_dicts:\n",
    "        cumsum_total += reward_dict['total']\n",
    "        accumulated['total'].append(cumsum_total)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: All components separately\n",
    "    ax1 = axes[0]\n",
    "    for comp in components:\n",
    "        ax1.plot(accumulated[comp], label=comp, linewidth=2)\n",
    "    \n",
    "    ax1.set_title('Accumulated Reward by Component', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Time Steps')\n",
    "    ax1.set_ylabel('Accumulated Reward')\n",
    "    ax1.legend(loc='best', framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Total accumulated reward\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(accumulated['total'], color='black', linewidth=3, label='Total')\n",
    "    ax2.set_title('Total Accumulated Reward', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Time Steps')\n",
    "    ax2.set_ylabel('Accumulated Reward')\n",
    "    ax2.legend(loc='best', framealpha=0.9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1de327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    evaluate_policy(client, policy, max_steps=1000, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vbfvpl5mp6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze training metrics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# Find the most recent metrics file\n",
    "metrics_files = glob('./models/actor-critic-ppo/training_metrics_*.json')\n",
    "if metrics_files:\n",
    "    latest_metrics = max(metrics_files, key=os.path.getctime)\n",
    "    print(f\"Loading metrics from: {latest_metrics}\")\n",
    "    \n",
    "    with open(latest_metrics, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    iterations = metrics['iterations']\n",
    "    \n",
    "    # Extract data\n",
    "    iters = [d['iteration'] for d in iterations]\n",
    "    success_rates = [d['success_rate'] for d in iterations]\n",
    "    avg_rewards = [d['avg_reward'] for d in iterations]\n",
    "    policy_losses = [d['policy_loss'] for d in iterations]\n",
    "    value_losses = [d['value_loss'] for d in iterations]\n",
    "    ratio_means = [d['ratio_mean'] for d in iterations]\n",
    "    ratio_mins = [d['ratio_min'] for d in iterations]\n",
    "    ratio_maxs = [d['ratio_max'] for d in iterations]\n",
    "    clipped_fracs = [d['clipped_frac'] for d in iterations]\n",
    "    entropies = [d['entropy'] for d in iterations]\n",
    "    \n",
    "    # Create comprehensive plots\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    fig.suptitle('PPO Training Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Success Rate\n",
    "    axes[0, 0].plot(iters, success_rates, linewidth=2, color='green')\n",
    "    axes[0, 0].set_title('Landing Success Rate', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Success Rate')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    \n",
    "    # Plot 2: Average Reward\n",
    "    axes[0, 1].plot(iters, avg_rewards, linewidth=2, color='blue')\n",
    "    axes[0, 1].set_title('Average Episode Reward', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('Average Reward')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Policy Loss\n",
    "    axes[1, 0].plot(iters, policy_losses, linewidth=2, color='red')\n",
    "    axes[1, 0].set_title('Policy Loss', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Iteration')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Value Loss\n",
    "    axes[1, 1].plot(iters, value_losses, linewidth=2, color='orange')\n",
    "    axes[1, 1].set_title('Value Loss', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Iteration')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Ratio Statistics\n",
    "    axes[2, 0].plot(iters, ratio_means, linewidth=2, label='Mean', color='purple')\n",
    "    axes[2, 0].fill_between(iters, ratio_mins, ratio_maxs, alpha=0.3, color='purple', label='Min-Max Range')\n",
    "    axes[2, 0].axhline(y=1.0, color='black', linestyle='--', linewidth=1, label='No Change (1.0)')\n",
    "    axes[2, 0].axhline(y=1.2, color='red', linestyle=':', linewidth=1, label='Clip Bound (1.2)')\n",
    "    axes[2, 0].axhline(y=0.8, color='red', linestyle=':', linewidth=1, label='Clip Bound (0.8)')\n",
    "    axes[2, 0].set_title('Policy Ratio Statistics', fontweight='bold')\n",
    "    axes[2, 0].set_xlabel('Iteration')\n",
    "    axes[2, 0].set_ylabel('Ratio (π_new / π_old)')\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Clipped Fraction & Entropy\n",
    "    ax6 = axes[2, 1]\n",
    "    ax6_twin = ax6.twinx()\n",
    "    \n",
    "    line1 = ax6.plot(iters, clipped_fracs, linewidth=2, color='brown', label='Clipped Fraction')\n",
    "    ax6.set_xlabel('Iteration')\n",
    "    ax6.set_ylabel('Clipped Fraction', color='brown')\n",
    "    ax6.tick_params(axis='y', labelcolor='brown')\n",
    "    ax6.set_ylim([0, 1])\n",
    "    \n",
    "    line2 = ax6_twin.plot(iters, entropies, linewidth=2, color='teal', label='Entropy')\n",
    "    ax6_twin.set_ylabel('Entropy', color='teal')\n",
    "    ax6_twin.tick_params(axis='y', labelcolor='teal')\n",
    "    \n",
    "    ax6.set_title('Clipping & Exploration', fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax6.legend(lines, labels, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nHyperparameters:\")\n",
    "    for key, value in metrics['hyperparameters'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n\\nFinal Performance (last 100 iterations):\")\n",
    "    last_100 = iterations[-100:]\n",
    "    print(f\"  Success Rate: {np.mean([d['success_rate'] for d in last_100]):.1%}\")\n",
    "    print(f\"  Avg Reward: {np.mean([d['avg_reward'] for d in last_100]):.1f}\")\n",
    "    print(f\"  Avg Steps: {np.mean([d['avg_steps'] for d in last_100]):.1f}\")\n",
    "    \n",
    "    print(f\"\\n\\nPPO-Specific Metrics (last 100 iterations):\")\n",
    "    print(f\"  Ratio Mean: {np.mean([d['ratio_mean'] for d in last_100]):.3f}\")\n",
    "    print(f\"  Clipped Fraction: {np.mean([d['clipped_frac'] for d in last_100]):.1%}\")\n",
    "    print(f\"  Entropy: {np.mean([d['entropy'] for d in last_100]):.4f}\")\n",
    "    \n",
    "    # Find convergence point (when success rate exceeds 50%)\n",
    "    convergence_iter = next((d['iteration'] for d in iterations if d['success_rate'] >= 0.5), None)\n",
    "    if convergence_iter:\n",
    "        print(f\"\\n\\nConvergence Point (50% success): Iteration {convergence_iter}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"No training metrics found. Run training first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
