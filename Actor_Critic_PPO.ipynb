{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6ccdfc",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494734f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from rl_helpers.scalers import *\n",
    "from torch.distributions import Bernoulli\n",
    "from delivery_drone.game.socket_client import DroneGameClient, DroneState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DroneGameClient(port=5555)\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50642911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_helpers.scalers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169f397",
   "metadata": {},
   "source": [
    "### State Variables\n",
    "\n",
    "| Variable | Min | Max | Typical Range | Critical Threshold |\n",
    "|----------|-----|-----|---------------|-------------------|\n",
    "| drone_x | 0 | 1 | [0, 1] | - |\n",
    "| drone_y | 0 | 1 | [0, 1] | - |\n",
    "| drone_vx | -2.5 | 2.5 | [-1.5, 1.5] | - |\n",
    "| drone_vy | -2.5 | 3.0 | [-0.5, 2.5] | - |\n",
    "| drone_angle | -1 | 1 | [-0.5, 0.5] | < 0.111 for landing |\n",
    "| drone_angular_vel | -1.5 | 1.5 | [-0.8, 0.8] | - |\n",
    "| drone_fuel | 0 | 1 | [0, 1] | Episode ends at 0 |\n",
    "| platform_x | 0 | 1 | [0.1, 0.9] | - |\n",
    "| platform_y | 0 | 1 | [0.58, 0.92] | Platform spawns at [350, 550]px |\n",
    "| distance_to_platform | 0 | 1.41 | [0, 1.2] | - |\n",
    "| dx_to_platform | -1.125 | 1.125 | [-1, 1] | < ±0.0625 for landing |\n",
    "| dy_to_platform | -1.083 | 1.083 | [-0.5, 0.8] | - |\n",
    "| speed | 0 | 3.9 | [0, 3.0] | < 0.3 for landing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_velocity_alignment(state: DroneState):\n",
    "    \"\"\"\n",
    "    Calculate how well the drone's velocity is aligned with optimal direction to platform.\n",
    "    Returns cosine similarity: 1.0 = perfect alignment, -1.0 = opposite direction\n",
    "    \"\"\"\n",
    "    # Optimal direction: from drone to platform\n",
    "    optimal_dx = state.dx_to_platform\n",
    "    optimal_dy = state.dy_to_platform\n",
    "    optimal_norm = math.sqrt(optimal_dx**2 + optimal_dy**2)\n",
    "\n",
    "    if optimal_norm < 1e-6:  # Already at platform\n",
    "        return 1.0\n",
    "\n",
    "    optimal_dx /= optimal_norm\n",
    "    optimal_dy /= optimal_norm\n",
    "\n",
    "    # Current velocity direction\n",
    "    velocity_norm = state.speed\n",
    "    if velocity_norm < 1e-6:  # Not moving\n",
    "        return 0.0\n",
    "\n",
    "    velocity_dx = state.drone_vx / velocity_norm\n",
    "    velocity_dy = state.drone_vy / velocity_norm\n",
    "\n",
    "    # Cosine similarity\n",
    "    return velocity_dx * optimal_dx + velocity_dy * optimal_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity-magnitude-weighted distance reward\n",
    "def calc_reward(state: DroneState, prev_state: DroneState = None):\n",
    "    rewards = {}\n",
    "    total_reward = 0\n",
    "\n",
    "    # Time penalty\n",
    "    minimum_time_penalty = 0\n",
    "    maximum_time_penalty = 1\n",
    "    rewards['time_penalty'] = -inverse_quadratic(\n",
    "        state.distance_to_platform,\n",
    "        decay=100,\n",
    "        scaler=maximum_time_penalty-minimum_time_penalty) - minimum_time_penalty\n",
    "\n",
    "    # Distance-based time penalty\n",
    "    # Penalty gets smaller as drone gets closer to platform\n",
    "    # Uses inverse quadratic function: higher penalty when far, reduces as distance decreases\n",
    "    # Minimum penalty of 0.5, maximum of 2.0 per timestep\n",
    "    total_reward += -0.5#rewards['time_penalty']\n",
    "\n",
    "    velocity_alignment = calc_velocity_alignment(state)\n",
    "    dist = state.distance_to_platform\n",
    "    \n",
    "    rewards['distance'] = 0\n",
    "    rewards['hovering'] = 0\n",
    "\n",
    "    if prev_state is not None:\n",
    "        distance_delta = prev_state.distance_to_platform - state.distance_to_platform\n",
    "        speed = state.speed\n",
    "        \n",
    "        # Calculate velocity toward platform\n",
    "        if dist > 1e-6:\n",
    "            velocity_toward_platform = (\n",
    "                state.drone_vx * state.dx_to_platform +\n",
    "                state.drone_vy * state.dy_to_platform\n",
    "            ) / dist\n",
    "        else:\n",
    "            velocity_toward_platform = 0.0\n",
    "        \n",
    "        MIN_MEANINGFUL_SPEED = 0.15  # Require meaningful velocity\n",
    "        \n",
    "        # Only reward if FAST and ALIGNED and making PROGRESS\n",
    "        if speed >= MIN_MEANINGFUL_SPEED and velocity_toward_platform > 0.1 and dist > 0.065:\n",
    "            # Good: Moving fast toward platform\n",
    "            speed_multiplier = 1.0 + speed * 2.0\n",
    "            rewards['distance'] = float(np.clip(distance_delta * 1000 * speed_multiplier, -2, 5))\n",
    "        elif distance_delta < -0.001:\n",
    "            # BAD: Moving away from platform (distance increasing)\n",
    "            rewards['distance'] = -2.0 * abs(distance_delta) * 1000  # Harsh penalty\n",
    "            rewards['hovering'] = 0  # Don't double-penalize\n",
    "        elif speed < 0.05:\n",
    "            # Hovering\n",
    "            rewards['hovering'] = -1.0\n",
    "        elif speed < MIN_MEANINGFUL_SPEED:\n",
    "            # Too slow\n",
    "            rewards['hovering'] = -0.3\n",
    "        else:\n",
    "            rewards['distance'] = 0.0\n",
    "            \n",
    "    total_reward += rewards['distance']\n",
    "    total_reward += rewards['hovering']\n",
    "\n",
    "    # Angle penalty (define a distance based max threshold)\n",
    "    abs_angle = abs(state.drone_angle)\n",
    "    max_angle = 0.20\n",
    "    max_permissible_angle = ((max_angle-0.111)*dist) + 0.111\n",
    "    excess = abs_angle - max_permissible_angle # excess angle\n",
    "    rewards['angle'] = -max(excess, 0) # maximum reward is 0 (we dont want it to reward hack for stability)\n",
    "\n",
    "    total_reward += rewards['angle']\n",
    "\n",
    "    # Speed - penalize excessive speed\n",
    "    rewards['speed'] = 0\n",
    "    speed = state.speed\n",
    "    max_speed = 0.6\n",
    "    if dist < 1:\n",
    "        rewards['speed'] = -2 * max(speed-0.1, 0)\n",
    "    else:\n",
    "        rewards['speed'] = -1 * max(speed-max_speed, 0)\n",
    "    total_reward += rewards['speed']\n",
    "\n",
    "    # Penalize being below platform\n",
    "    rewards['vertical_position'] = 0\n",
    "    if state.dy_to_platform > 0:  # Platform is below drone (drone is above - GOOD)\n",
    "        rewards['vertical_position'] = 0\n",
    "    else:  # Drone is below platform (BAD!)\n",
    "        rewards['vertical_position'] = state.dy_to_platform * 4.0  # Negative penalty\n",
    "    total_reward += rewards['vertical_position']\n",
    "\n",
    "    # Terminal\n",
    "    rewards['terminal'] = 0\n",
    "    if state.landed:\n",
    "        rewards['terminal'] = 800.0 + state.drone_fuel * 100.0\n",
    "    elif state.crashed:\n",
    "        rewards['terminal'] = -200.0\n",
    "        # Extra penalty for crashing far from target\n",
    "        if state.distance_to_platform > 0.3:\n",
    "            rewards['terminal'] -= 100.0\n",
    "    total_reward += rewards['terminal']\n",
    "\n",
    "    rewards['total'] = total_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset()\n",
    "state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a81352",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state = state if state else None\n",
    "client.step(\n",
    "    dict(\n",
    "        main_thrust=0,\n",
    "        left_thrust=0,\n",
    "        right_thrust=0\n",
    "    )\n",
    ")\n",
    "state = client.get_state()\n",
    "display(state.__dict__)\n",
    "calc_reward(state, prev_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_array(state, device='cpu'):\n",
    "    \"\"\"Convert DroneState dataclass to numpy array\"\"\"\n",
    "    data = np.array([\n",
    "        state.drone_x,\n",
    "        state.drone_y,\n",
    "        state.drone_vx,\n",
    "        state.drone_vy,\n",
    "        state.drone_angle,\n",
    "        state.drone_angular_vel,\n",
    "        state.drone_fuel,\n",
    "        state.platform_x,\n",
    "        state.platform_y,\n",
    "        state.distance_to_platform,\n",
    "        state.dx_to_platform,\n",
    "        state.dy_to_platform,\n",
    "        state.speed,\n",
    "        float(state.landed),\n",
    "        float(state.crashed)\n",
    "    ])\n",
    "    \n",
    "    return torch.tensor(data, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneGamerBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state, device=device)\n",
    "        \n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneTeacherBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) # output will be just a scalar\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state, device=device)\n",
    "        \n",
    "        return (self.network(state)\n",
    "                .squeeze(-1)) # this will convert the tensor from [B, 1] to [B,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abce185",
   "metadata": {},
   "source": "## Proximal Policy Optimization (PPO): How It Works\n\nPPO is an advanced Actor-Critic method that solves a critical problem: **how to update the policy safely without accidentally destroying what it has already learned**.\n\n### The Core Problem PPO Solves\n\n**In Basic Actor-Critic:**\n- One bad gradient update can collapse the policy (make it deterministic or terrible)\n- If we take too large a step, the policy might never recover\n- No mechanism to prevent catastrophic updates\n\n**PPO's solution:** **Clip the policy updates** to stay within a \"trust region\" - only allow small, controlled changes.\n\n---\n\n## Key Innovation: The Clipped Objective\n\n### 1. Policy Ratio\n\nPPO tracks how much the policy has changed by computing a ratio:\n\n$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$$\n\n**What this ratio means:**\n- $r_t = 1.0$: Policy unchanged for this state-action pair\n- $r_t > 1.0$: New policy makes action $a_t$ MORE likely\n- $r_t < 1.0$: New policy makes action $a_t$ LESS likely\n\n### 2. Clipped Surrogate Objective\n\nInstead of the standard policy gradient loss, PPO uses:\n\n$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\cdot A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot A_t\\right)\\right]$$\n\nWhere:\n- $A_t$ = Advantage (GAE: how good this action was vs average)\n- $\\epsilon$ = Clipping parameter (typically **0.2**)\n- $\\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)$ = Constrain ratio to $[0.8, 1.2]$\n\n### 3. What the Clipping Does\n\nThe `min()` creates a **pessimistic bound** - we take the worst-case of the two objectives:\n\n**Case 1: Positive Advantage ($A_t > 0$)** - Good action, want to increase probability\n\n| Ratio $r_t$ | Effect | Clipping Active? |\n|-------------|--------|------------------|\n| $r_t < 0.8$ | Increase probability | No (safe) |\n| $0.8 \\leq r_t \\leq 1.2$ | Increase probability | No (safe) |\n| $r_t > 1.2$ | **STOP increasing** | **Yes** (already changed +20%) |\n\n**Case 2: Negative Advantage ($A_t < 0$)** - Bad action, want to decrease probability\n\n| Ratio $r_t$ | Effect | Clipping Active? |\n|-------------|--------|------------------|\n| $r_t < 0.8$ | **STOP decreasing** | **Yes** (already changed -20%) |\n| $0.8 \\leq r_t \\leq 1.2$ | Decrease probability | No (safe) |\n| $r_t > 1.2$ | Decrease probability | No (safe) |\n\n**The genius:** Clipping prevents the policy from changing too drastically. If an action probability has already changed by ±20%, we stop pushing it further.\n\n---\n\n## PPO Algorithm Flow\n\n### Phase 1: Rollout Collection (Like REINFORCE)\n\n```\n1. Run current policy π_θ_old for N steps across parallel games\n2. Store: states, actions, log_probs, rewards, dones, values\n3. Compute GAE advantages for variance reduction\n```\n\n### Phase 2: Multi-Epoch Updates (Key Difference!)\n\nUnlike Actor-Critic (1 update per transition), PPO reuses data:\n\n```\nFor epoch in 1..K (typically K=4-10):\n    Shuffle collected data into minibatches\n    \n    For each minibatch:\n        # Compute new policy probabilities\n        new_log_probs = log π_θ(actions | states)\n        \n        # Policy ratio\n        ratio = exp(new_log_probs - old_log_probs)\n        \n        # Clipped objective\n        surr1 = ratio * advantages\n        surr2 = clip(ratio, 1-ε, 1+ε) * advantages\n        policy_loss = -min(surr1, surr2).mean()\n        \n        # Entropy bonus for exploration\n        entropy = -sum(probs * log(probs))\n        policy_loss -= entropy_coef * entropy\n        \n        # Update policy\n        policy_optimizer.step()\n        \n        # Value loss (simple MSE)\n        value_loss = (values - returns)²\n        critic_optimizer.step()\n```\n\n### Phase 3: Replace Old Policy\n\n```\nθ_old ← θ  # Save current policy for next rollout\n```\n\n---\n\n## Generalized Advantage Estimation (GAE)\n\nPPO typically uses GAE instead of simple TD error for lower variance:\n\n### Simple TD Error (Actor-Critic):\n$$A_t = \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\n### GAE (PPO):\n$$A_t^{\\text{GAE}} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n\nWhere $\\lambda \\in [0, 1]$ controls bias-variance tradeoff:\n- $\\lambda = 0$: Pure TD (low variance, high bias)\n- $\\lambda = 1$: Monte Carlo (high variance, low bias)  \n- $\\lambda = 0.95$: **PPO sweet spot** ✓\n\n**Practical computation (backward pass):**\n```python\nadvantages = []\ngae = 0\nfor t in reversed(range(T)):\n    delta = rewards[t] + gamma * values[t+1] - values[t]\n    gae = delta + gamma * lambda_ * gae\n    advantages.insert(0, gae)\n```\n\n---\n\n## PPO vs Actor-Critic vs REINFORCE\n\n| Feature | REINFORCE | Actor-Critic | **PPO** |\n|---------|-----------|--------------|---------|\n| **Update frequency** | End of episode | Every step | Every N steps |\n| **Data reuse** | No | No | **Yes (K epochs)** ✓ |\n| **Variance** | Very high | Medium | **Low (GAE)** ✓ |\n| **Stability** | Unstable | Can be unstable | **Very stable (clipping)** ✓ |\n| **Sample efficiency** | Poor | Good | **Best** ✓ |\n| **Prevents collapse** | No | No | **Yes (trust region)** ✓ |\n\n---\n\n## Why PPO Wins\n\n### 1. **Prevents Policy Collapse**\n\n**Without clipping:**\n```python\n# One huge gradient can ruin the policy\nloss = -log_prob * advantage  # No bounds!\n# If advantage is +100, gradient is HUGE → policy becomes deterministic\n```\n\n**With clipping:**\n```python\nratio = new_prob / old_prob\nclipped_ratio = clip(ratio, 0.8, 1.2)\nloss = -min(ratio * advantage, clipped_ratio * advantage)\n# Even if advantage is +100, change is limited to ±20%\n```\n\n### 2. **Data Efficiency**\n\n- **Actor-Critic**: Use each transition once → throw away\n- **PPO**: Reuse data 4-10 times → fewer environment interactions\n\n### 3. **Stable Across Tasks**\n\n- Works out-of-the-box on many tasks\n- Requires minimal hyperparameter tuning\n- Industry standard (OpenAI Five, AlphaStar, robotics)\n\n---\n\n## Mathematical Intuition: Why min()?\n\n```python\nL = -min(ratio * A, clip(ratio, 1-ε, 1+ε) * A)\n     ↑\n   Pessimistic: take minimum (worst-case) of two terms\n```\n\n**If $A_t > 0$ (good action):**\n- Unclipped: $r_t \\cdot A$ encourages increasing probability\n- Clipped: Caps increase once $r_t > 1.2$\n- **min() chooses:** Once ratio exceeds 1.2, gradient becomes zero (stops increasing)\n\n**If $A_t < 0$ (bad action):**\n- Unclipped: $r_t \\cdot A$ (negative) encourages decreasing probability\n- Clipped: Caps decrease once $r_t < 0.8$  \n- **min() chooses more negative:** Once ratio drops below 0.8, gradient becomes zero (stops decreasing)\n\nThe `min()` is **conservative** - it says: \"Don't be overconfident about policy improvements.\"\n\n---\n\n## PPO Hyperparameters\n\n```python\n# Policy\nlearning_rate_actor = 3e-4       # Standard PPO LR\nclip_epsilon = 0.2               # Trust region (±20%)\n\n# Value function\nlearning_rate_critic = 1e-3      # Often higher than actor\nvalue_loss_coef = 0.5            # Weight of critic loss\n\n# Training\nnum_epochs = 10                  # Reuse data this many times\nminibatch_size = 64              # For SGD updates\nrollout_steps = 2048             # Steps to collect before update\n\n# Advantage\ngamma = 0.99                     # Discount factor\ngae_lambda = 0.95                # GAE parameter\n\n# Regularization\nentropy_coef = 0.01              # Exploration bonus\nmax_grad_norm = 0.5              # Gradient clipping\n```\n\n### Key Effects\n\n| Parameter | Too Low | Too High | Sweet Spot |\n|-----------|---------|----------|------------|\n| `clip_epsilon` | Policy barely changes | Allows catastrophic updates | **0.2** |\n| `num_epochs` | Wastes data | Overfits to old data | **4-10** |\n| `gae_lambda` | High bias | High variance | **0.95** |\n| `entropy_coef` | No exploration | Too random | **0.01** |\n\n---\n\n## When to Use PPO\n\n### ✅ Use PPO when:\n- You want **stable, reliable training** (most important!)\n- You can collect batches of data before updating\n- Working on **continuous control** (robotics, drone landing)\n- Want **sample efficiency** (fewer env interactions)\n- Need algo that **works out-of-the-box**\n\n### ❌ Use Actor-Critic when:\n- Need **immediate online learning** (can't wait for batches)\n- Environment is cheap to simulate (sample efficiency less critical)\n- Prefer simplicity over performance\n\n---\n\n## Summary\n\n**PPO Formula:**\n> Trust regions through clipping - update safely!\n\n**Three Keys:**\n1. **Clipped objective:** Bound policy changes to ±20%\n2. **Multiple epochs:** Reuse data 4-10 times  \n3. **GAE advantages:** Low-variance advantage estimates\n\n**Why it's the industry default:**\n- Stable (no policy collapse)\n- Sample efficient (reuses data)\n- Robust (works across many tasks)\n\nFor **drone landing**: PPO will likely outperform basic Actor-Critic due to superior stability and data efficiency!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2259bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}