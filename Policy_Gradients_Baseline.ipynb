{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ed65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "from delivery_drone.game.socket_client import DroneGameClient, DroneState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacda758",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DroneGameClient(port=5555)\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbb8f2",
   "metadata": {},
   "source": [
    "### Going from `state` -> `reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scaler(value, scaler):\n",
    "    # Does value * scaler\n",
    "    return value * scaler\n",
    "\n",
    "def gausian_scaler(value, sigma=0.1, scaler=1):\n",
    "    return scaler * math.exp(-value**2/(2*sigma**2))\n",
    "\n",
    "def exponential_decay(value, decay=10, scaler=1):\n",
    "    return scaler * math.exp(-decay*(abs(value)))\n",
    "\n",
    "def inverse_quadratic(value, decay=10, scaler=1):\n",
    "    return scaler * (1/(1+(decay*(value**2))))\n",
    "\n",
    "def inverse_linear(value, decay=10, scaler=1):\n",
    "    return scaler * (1/(1+(decay*abs(value))))\n",
    "\n",
    "def scaled_shifted_negative_sigmoid(value, sigma=10, scaler=1):\n",
    "    return scaler * (1/(1+math.exp(sigma*(value-0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7561a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    client.step(\n",
    "        {\n",
    "            'main_thrust': 0,\n",
    "            'left_thrust': 1,\n",
    "            'right_thrust': 0\n",
    "        }, 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb69ec",
   "metadata": {},
   "source": [
    "### State Variables\n",
    "\n",
    "| Variable | Min | Max | Typical Range | Critical Threshold |\n",
    "|----------|-----|-----|---------------|-------------------|\n",
    "| drone_x | 0 | 1 | [0, 1] | - |\n",
    "| drone_y | 0 | 1 | [0, 1] | - |\n",
    "| drone_vx | -2.5 | 2.5 | [-1.5, 1.5] | - |\n",
    "| drone_vy | -2.5 | 3.0 | [-0.5, 2.5] | - |\n",
    "| drone_angle | -1 | 1 | [-0.5, 0.5] | < 0.111 for landing |\n",
    "| drone_angular_vel | -1.5 | 1.5 | [-0.8, 0.8] | - |\n",
    "| drone_fuel | 0 | 1 | [0, 1] | Episode ends at 0 |\n",
    "| platform_x | 0 | 1 | [0.1, 0.9] | - |\n",
    "| platform_y | 0 | 1 | [0.58, 0.92] | Platform spawns at [350, 550]px |\n",
    "| distance_to_platform | 0 | 1.41 | [0, 1.2] | - |\n",
    "| dx_to_platform | -1.125 | 1.125 | [-1, 1] | < ±0.0625 for landing |\n",
    "| dy_to_platform | -1.083 | 1.083 | [-0.5, 0.8] | - |\n",
    "| speed | 0 | 3.9 | [0, 3.0] | < 0.3 for landing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458935de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_velocity_alignment(state: DroneState):\n",
    "    \"\"\"\n",
    "    Calculate how well aligned the drone's velocity is with the optimal direction.\n",
    "    Returns cosine similarity: 1.0 = perfect alignment, -1.0 = opposite direction, 0 = perpendicular\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Velocity vector\n",
    "    vx = state.drone_vx\n",
    "    vy = state.drone_vy\n",
    "    \n",
    "    # Direction vector from drone to platform\n",
    "    dx = state.dx_to_platform\n",
    "    dy = state.dy_to_platform\n",
    "    \n",
    "    # Calculate magnitudes\n",
    "    velocity_magnitude = math.sqrt(vx**2 + vy**2)\n",
    "    direction_magnitude = math.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if velocity_magnitude == 0:\n",
    "        return 0  # Stationary drone, no alignment\n",
    "    \n",
    "    if direction_magnitude == 0:\n",
    "        return 0  # Already at platform\n",
    "    \n",
    "    # Dot product\n",
    "    dot_product = vx * dx + vy * dy\n",
    "    \n",
    "    # Cosine similarity: cos(θ) = (v · d) / (|v| * |d|)\n",
    "    cos_theta = dot_product / (velocity_magnitude * direction_magnitude)\n",
    "    \n",
    "    return cos_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(state: DroneState):\n",
    "    rewards = {}\n",
    "    total_reward = 0\n",
    "    \n",
    "    time_step = state.steps\n",
    "    \n",
    "    # Time penalty\n",
    "    minimum_time_penalty = 0.3\n",
    "    maximum_time_penalty = 1\n",
    "    rewards['time_penalty'] = -inverse_quadratic(\n",
    "        state.distance_to_platform, \n",
    "        decay=50, \n",
    "        scaler=maximum_time_penalty-minimum_time_penalty) - minimum_time_penalty\n",
    "    \n",
    "    # Distance-based time penalty\n",
    "    # Penalty gets smaller as drone gets closer to platform\n",
    "    # Uses inverse quadratic function: higher penalty when far, reduces as distance decreases\n",
    "    # Minimum penalty of 0.5, maximum of 2.0 per timestep\n",
    "    total_reward += rewards['time_penalty']\n",
    "    \n",
    "    velocity_alignment = calc_velocity_alignment(state)\n",
    "    dist = state.distance_to_platform\n",
    "    \n",
    "    rewards['distance'] = 0\n",
    "    rewards['velocity_alignment'] = 0\n",
    "\n",
    "    if dist > 0.065 and state.dy_to_platform > 0:  # ADD: only if drone ABOVE platform\n",
    "        rewards['distance'] = int(velocity_alignment > 0) * state.speed * scaled_shifted_negative_sigmoid(dist, scaler=4.5)\n",
    "        \n",
    "        if velocity_alignment > 0:\n",
    "            rewards['velocity_alignment'] = 0.5\n",
    "\n",
    "    total_reward += rewards['distance']\n",
    "    total_reward += rewards['velocity_alignment']\n",
    "    \n",
    "    # Angle penalty (define a distance based max threshold)\n",
    "    abs_angle = abs(state.drone_angle)\n",
    "    max_angle = 0.20\n",
    "    max_permissible_angle = ((max_angle-0.111)*dist) + 0.111\n",
    "    excess = abs_angle - max_permissible_angle # excess angle\n",
    "    rewards['angle'] = -max(excess, 0) # maximum reward is 0 (we dont want it to reward hack for stability)\n",
    "    \n",
    "    total_reward += rewards['angle']\n",
    "    \n",
    "    # Speed - penalize excessive speed\n",
    "    rewards['speed'] = 0\n",
    "    speed = state.speed\n",
    "    max_speed = 0.4\n",
    "    if dist < 1:\n",
    "        rewards['speed'] = -2 * max(speed-0.1, 0)\n",
    "    else:\n",
    "        rewards['speed'] = -1 * max(speed-max_speed, 0)\n",
    "    total_reward += rewards['speed']\n",
    "    \n",
    "    # Penalize being below platform\n",
    "    rewards['vertical_position'] = 0\n",
    "    if state.dy_to_platform > 0:  # Platform is below drone (drone is above - GOOD)\n",
    "        rewards['vertical_position'] = 0\n",
    "    else:  # Drone is below platform (BAD!)\n",
    "        rewards['vertical_position'] = state.dy_to_platform * 4.0  # Negative penalty\n",
    "    total_reward += rewards['vertical_position']\n",
    "    \n",
    "    # Terminal\n",
    "    rewards['terminal'] = 0\n",
    "    if state.landed:\n",
    "        rewards['terminal'] = 500.0 + state.drone_fuel * 100.0\n",
    "    elif state.crashed:\n",
    "        rewards['terminal'] = -200.0\n",
    "        # Extra penalty for crashing far from target\n",
    "        if state.distance_to_platform > 0.3:\n",
    "            rewards['terminal'] -= 100.0\n",
    "    total_reward += rewards['terminal']\n",
    "    \n",
    "    rewards['total'] = total_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = client.reset(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975e26c",
   "metadata": {},
   "source": [
    "### Going from `reward` -> `loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ae5af",
   "metadata": {},
   "source": [
    "There are several approaches to convert RL rewards into a loss function for neural networks:\n",
    "1. Policy Gradient Methods (REINFORCE, PPO, A3C)\n",
    "Maximize expected reward by minimizing negative log-likelihood weighted by returns:\n",
    "```py\n",
    "loss = -log_prob(action) * reward\n",
    "# Or with advantage:\n",
    "loss = -log_prob(action) * advantage\n",
    "```\n",
    "2. Q-Learning / DQN\n",
    "Minimize TD (Temporal Difference) error:\n",
    "```py\n",
    "# Predict Q-value for action taken\n",
    "q_predicted = model(state)[action]\n",
    "\n",
    "# Target Q-value (Bellman equation)\n",
    "q_target = reward + gamma * max(model(next_state))\n",
    "\n",
    "# MSE loss\n",
    "loss = (q_predicted - q_target)^2\n",
    "```\n",
    "3. Actor-Critic Methods (A2C, SAC)\n",
    "Two separate losses:\n",
    "```py\n",
    "# Actor loss (policy)\n",
    "actor_loss = -log_prob(action) * advantage\n",
    "\n",
    "# Critic loss (value function)\n",
    "critic_loss = (value_predicted - value_target)^2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771feb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_prob_loss(action_probs, action_index, reward):\n",
    "    \"\"\"\n",
    "    Computes the negative log-probability loss for policy gradient methods.\n",
    "\n",
    "    Args:\n",
    "        action_probs (torch.Tensor): Tensor of probabilities for each action.\n",
    "        action_index (int): Index of the action taken.\n",
    "        reward (float): Scalar reward. Can be replaced by advantage for advantage-based methods.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = -torch.log(action_probs[action_index]) * reward\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_loss(q_predicted, reward, gamma, q_next_state, done):\n",
    "    \"\"\"\n",
    "    Computes the Q-learning loss using TD error.\n",
    "    \n",
    "    Args:\n",
    "        q_predicted (torch.Tensor): Predicted Q-value for the taken action\n",
    "        reward (float): Immediate reward received\n",
    "        gamma (float): Discount factor for future rewards\n",
    "        q_next_state (torch.Tensor): Predicted Q-values for next state\n",
    "        done (bool): Whether the episode has ended\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The computed TD error loss\n",
    "    \"\"\"\n",
    "    # If done, next state value is 0, otherwise it's the max Q-value of next state\n",
    "    next_value = 0 if done else torch.max(q_next_state)\n",
    "    \n",
    "    # Compute target Q-value using Bellman equation\n",
    "    q_target = reward + gamma * next_value\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    loss = (q_predicted - q_target.detach()) ** 2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4eb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = client.get_state(0)\n",
    "display(state.__dict__)\n",
    "\n",
    "calc_reward(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a2652",
   "metadata": {},
   "source": [
    "## Let's Create a Policy Network now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_array(state, device='cpu'):\n",
    "    \"\"\"Convert DroneState dataclass to numpy array\"\"\"\n",
    "    data = np.array([\n",
    "        state.drone_x,\n",
    "        state.drone_y,\n",
    "        state.drone_vx,\n",
    "        state.drone_vy,\n",
    "        state.drone_angle,\n",
    "        state.drone_angular_vel,\n",
    "        state.drone_fuel,\n",
    "        state.platform_x,\n",
    "        state.platform_y,\n",
    "        state.distance_to_platform,\n",
    "        state.dx_to_platform,\n",
    "        state.dy_to_platform,\n",
    "        state.speed,\n",
    "        float(state.landed),\n",
    "        float(state.crashed)\n",
    "    ])\n",
    "    \n",
    "    return torch.tensor(data, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneGamerBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state, device=device)\n",
    "        \n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88b404",
   "metadata": {},
   "source": [
    "## How do I train my policy:\n",
    "\n",
    "1. simple approach - Online Learning (Naive):\n",
    "```py\n",
    "state = Reset_game()\n",
    "for _ in range(training_steps):\n",
    "    probs = policy(state)\n",
    "    action = sample_from(probs)\n",
    "    state = game_update(action)\n",
    "    reward = calc_reward\n",
    "    loss = loss_fn(reward)\n",
    "    gradient_step(policy, loss)\n",
    "```\n",
    "**Problems**:\n",
    "- Too much Varience\n",
    "- Our policy will learn to do erratic movements\n",
    "\n",
    "(Not going to implement this BS)\n",
    "\n",
    "2. Episodes - (Less, but still, naive):\n",
    "**Core Idea**: _Take one episode, i.e, let the policy sample till the episode ends, which means either the drone crashed or landed._\n",
    "\n",
    "```py\n",
    "for _ in range(num_training_episode):\n",
    "    # Collect full episode`\n",
    "    states, actions, rewards = [], [], []\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        probs = policy(state)\n",
    "        action = sample_from(probs)\n",
    "        state = game_update(action)\n",
    "\n",
    "        reward = calc_reward(state)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    total_loss = sum([loss_fn(reward) for reward in rewards])\n",
    "    gradient_step(policy, total_loss)\n",
    "```\n",
    "\n",
    "-> This is way better than the previous one, because we will at least optimize for winning (or, negatively, for losing) the game\n",
    "\n",
    "**Problems**: _Still high varience, the policy may learn how to win, but it may also reinforce erratic behaviour_\n",
    "\n",
    "---\n",
    "\n",
    "My point is that there are many ways to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b6eb8",
   "metadata": {},
   "source": [
    "#### Policy Gradient with Baseline\n",
    "\n",
    "**Core Idea**:\n",
    "\n",
    "1. Collect multiple episodes\n",
    "2. Calculate the mean of all the returns (called _Baseline_)\n",
    "3. Calculate advantage of each episode (i.e., subtract all episode's returns with the baseline)\n",
    "4. Compute Loss of the actions weighted against the advantage.\n",
    "5. Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DroneGamerBoi().to(device) # initialize our policy\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48faf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configurations\n",
    "num_iterations = 1000\n",
    "num_episodes = client.num_games\n",
    "\n",
    "bellman_gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(client: DroneGameClient, policy: DroneGamerBoi, max_steps=300):\n",
    "    \"\"\"\n",
    "    Collect episodes with early stopping\n",
    "    \n",
    "    Args:\n",
    "        max_steps: Maximum steps per episode (default: 300)\n",
    "    \"\"\"\n",
    "    num_games = client.num_games\n",
    "    \n",
    "    \n",
    "    # Initialize storage\n",
    "    all_episodes = [{'states': [], 'actions': [], 'log_probs': [], 'rewards': [], 'done': False} \n",
    "                    for _ in range(num_games)]\n",
    "    \n",
    "    # Reset all games\n",
    "    game_states = [client.reset(game_id) for game_id in range(num_games)]\n",
    "    step_counts = [0] * num_games  # Track steps per game\n",
    "    \n",
    "    while not all(ep['done'] for ep in all_episodes):\n",
    "        # Batch active games\n",
    "        batch_states = []\n",
    "        active_game_ids = []\n",
    "        \n",
    "        for game_id in range(num_games):\n",
    "            if not all_episodes[game_id]['done']:\n",
    "                batch_states.append(state_to_array(game_states[game_id]))\n",
    "                active_game_ids.append(game_id)\n",
    "        \n",
    "        if len(batch_states) == 0:\n",
    "            break\n",
    "        \n",
    "        # Batched inference\n",
    "        batch_states_tensor = torch.stack(batch_states).to(device)\n",
    "        batch_action_probs = policy(batch_states_tensor)#.to('cpu')\n",
    "        batch_dist = Bernoulli(probs=batch_action_probs)\n",
    "        batch_actions = batch_dist.sample()\n",
    "        batch_log_probs = batch_dist.log_prob(batch_actions).sum(dim=1)\n",
    "        \n",
    "        # Execute actions\n",
    "        for i, game_id in enumerate(active_game_ids):\n",
    "            action = batch_actions[i]\n",
    "            log_prob = batch_log_probs[i]\n",
    "            \n",
    "            next_state, _, done, _ = client.step({\n",
    "                \"main_thrust\": int(action[0]),\n",
    "                \"left_thrust\": int(action[1]),\n",
    "                \"right_thrust\": int(action[2])\n",
    "            }, game_id)\n",
    "            \n",
    "            reward = calc_reward(next_state)\n",
    "            \n",
    "            # Store data\n",
    "            all_episodes[game_id]['states'].append(batch_states[i])\n",
    "            all_episodes[game_id]['actions'].append(action)\n",
    "            all_episodes[game_id]['log_probs'].append(log_prob)\n",
    "            all_episodes[game_id]['rewards'].append(reward['total'])\n",
    "            \n",
    "            # Update state and step count\n",
    "            game_states[game_id] = next_state\n",
    "            step_counts[game_id] += 1\n",
    "            \n",
    "            # Check done conditions\n",
    "            if done or step_counts[game_id] >= max_steps:\n",
    "                # Apply timeout penalty if hit max steps without landing\n",
    "                if step_counts[game_id] >= max_steps and not next_state.landed:\n",
    "                    all_episodes[game_id]['rewards'][-1] -= 500  # Timeout penalty\n",
    "                \n",
    "                all_episodes[game_id]['done'] = True\n",
    "    \n",
    "    # Return episodes\n",
    "    return [(ep['states'], ep['actions'], ep['log_probs'], ep['rewards']) \n",
    "            for ep in all_episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d50e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns (G_t) for each timestep based on the Bellman equation\n",
    "    \n",
    "    G_t = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Compute backwards (more efficient)\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0876b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_simple(client, policy, max_steps=300, game_id=0, temperature=0.5, iteration=0, fig_ax=None):\n",
    "    \"\"\"\n",
    "    Simple evaluation with static plots that reuse the same figure.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    policy.eval()\n",
    "    state = client.reset(game_id)\n",
    "    \n",
    "    history = {\n",
    "        'step': [], 'time_penalty': [], 'distance': [], 'velocity_alignment': [], \"vertical_position\": [],\n",
    "        'angle': [], 'speed': [], 'terminal': [], 'total': []\n",
    "    }\n",
    "    \n",
    "    accumulated = {\n",
    "        'time_penalty': 0, 'distance': 0, 'velocity_alignment': 0, \"vertical_position\": 0,\n",
    "        'angle': 0, 'speed': 0, \n",
    "        'terminal': 0, 'total': 0\n",
    "    }\n",
    "    \n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Run episode\n",
    "    while not done and steps < max_steps:\n",
    "        with torch.no_grad():\n",
    "            action_probs = policy(state)\n",
    "            \n",
    "        if temperature == 0:\n",
    "            action = (action_probs > 0.5).float()\n",
    "        else:\n",
    "            adjusted_probs = torch.pow(action_probs, 1.0 / temperature)\n",
    "            adjusted_probs = adjusted_probs / (adjusted_probs + torch.pow(1 - action_probs, 1.0 / temperature))\n",
    "            dist = Bernoulli(probs=adjusted_probs)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        next_state, _, done, _ = client.step({\n",
    "            \"main_thrust\": int(action[0]),\n",
    "            \"left_thrust\": int(action[1]),\n",
    "            \"right_thrust\": int(action[2])\n",
    "        }, game_id)\n",
    "        \n",
    "        reward = calc_reward(next_state)\n",
    "        \n",
    "        # Accumulate rewards\n",
    "        for key in accumulated.keys():\n",
    "            accumulated[key] += reward[key]\n",
    "        \n",
    "        # Store history\n",
    "        history['step'].append(steps)\n",
    "        for key in accumulated.keys():\n",
    "            history[key].append(accumulated[key])\n",
    "        \n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    # Create or reuse figure\n",
    "    if fig_ax is None:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    else:\n",
    "        fig, (ax1, ax2) = fig_ax\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "    \n",
    "    # Plot 1: All components\n",
    "    ax1.plot(history['step'], history['time_penalty'], label='time_penalty', linewidth=2)\n",
    "    ax1.plot(history['step'], history['distance'], label='distance', linewidth=2)\n",
    "    ax1.plot(history['step'], history['angle'], label='angle', linewidth=2)\n",
    "    ax1.plot(history['step'], history['speed'], label='speed', linewidth=2)\n",
    "    ax1.plot(history['step'], history['velocity_alignment'], label='velocity_alignment', linewidth=2)\n",
    "    ax1.plot(history['step'], history['vertical_position'], label='vertical_position', linewidth=2)\n",
    "    ax1.plot(history['step'], history['terminal'], label='terminal', linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Time Steps', fontsize=11)\n",
    "    ax1.set_ylabel('Accumulated Reward', fontsize=11)\n",
    "    ax1.set_title(f'Accumulated Reward by Component (Iter {iteration})', fontweight='bold', fontsize=12)\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Total\n",
    "    ax2.plot(history['step'], history['total'], color='black', linewidth=3)\n",
    "    ax2.set_xlabel('Time Steps', fontsize=11)\n",
    "    ax2.set_ylabel('Accumulated Reward', fontsize=11)\n",
    "    ax2.set_title(f'Total Accumulated Reward (Iter {iteration})', fontweight='bold', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add result annotation\n",
    "    status = \"[LANDED]\" if state.landed else \"[CRASHED]\"\n",
    "    color = 'green' if state.landed else 'red'\n",
    "    result_text = f\"{status} | Steps: {steps} | Total: {accumulated['total']:.1f} | Fuel: {state.drone_fuel:.1%}\"\n",
    "    \n",
    "    # Clear previous suptitle and add new one\n",
    "    fig.suptitle(result_text, fontsize=13, fontweight='bold', color=color, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the figure using IPython display\n",
    "    display(fig)\n",
    "    \n",
    "    return {\n",
    "        'landed': state.landed,\n",
    "        'steps': steps,\n",
    "        'total_reward': accumulated['total'],\n",
    "        'final_fuel': state.drone_fuel\n",
    "    }, (fig, (ax1, ax2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steepness = 0.65\n",
    "start = 300\n",
    "end = 500\n",
    "\n",
    "x = np.linspace(0, 1, num=num_iterations)\n",
    "step_schedule = np.round(start + (end - start) * x**steepness).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa23c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = policy.to(device)\n",
    "\n",
    "# Evaluation settings\n",
    "eval_interval = 10        # Evaluate every N iterations\n",
    "eval_temperature = 0.3    # Sampling temperature for eval\n",
    "\n",
    "# Shared figure object\n",
    "eval_fig_ax = None\n",
    "\n",
    "tqdm_iterations = trange(num_iterations, desc='', total=num_iterations)\n",
    "\n",
    "for iteration in tqdm_iterations:\n",
    "    max_steps = step_schedule[iteration]\n",
    "    \n",
    "    episodes = collect_episodes(client, policy, max_steps=max_steps)\n",
    "    \n",
    "    batch_log_probs = []\n",
    "    batch_returns = []\n",
    "    total_reward = 0\n",
    "    episode_lengths = []\n",
    "    num_successes = 0\n",
    "    \n",
    "    for states, actions, log_probs, rewards in episodes:\n",
    "        returns = compute_returns(rewards, gamma=bellman_gamma)\n",
    "        batch_log_probs.extend(log_probs)\n",
    "        batch_returns.extend(returns)\n",
    "        total_reward += sum(rewards)\n",
    "        episode_lengths.append(len(rewards))\n",
    "        if rewards[-1] > 0:\n",
    "            num_successes += 1\n",
    "    \n",
    "    # Train\n",
    "    returns_tensor = torch.tensor(batch_returns, dtype=torch.float32, device=device)\n",
    "    baseline = returns_tensor.mean()\n",
    "    advantages = (returns_tensor - baseline)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    log_probs_tensor = torch.stack(batch_log_probs)\n",
    "    loss = -(log_probs_tensor * advantages).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    tqdm_iterations.set_description(\n",
    "        f'Success: {num_successes}/{len(episodes)} | '\n",
    "        f'Baseline: {baseline.item():.1f} | '\n",
    "        f'Reward Std: {returns_tensor.std():.1f} | '\n",
    "        f'Avg Len: {sum(episode_lengths)/len(episode_lengths):.1f} | '\n",
    "        f'Loss: {loss.item():.4f} | '\n",
    "        f'Max Steps: {max_steps}'\n",
    "    )\n",
    "    \n",
    "    # Evaluation (plots stay visible, tqdm continues)\n",
    "    if (iteration + 1) % eval_interval == 0:\n",
    "        eval_result, eval_fig_ax = evaluate_policy_simple(\n",
    "            client, \n",
    "            policy, \n",
    "            max_steps=500,\n",
    "            temperature=eval_temperature,\n",
    "            iteration=iteration + 1,\n",
    "            game_id=0,\n",
    "            fig_ax=eval_fig_ax  # Pass existing figure\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(policy.state_dict(), './models/drone_policy_v1.4.pth')\n",
    "\n",
    "# Optionally, save the entire model (includes architecture)\n",
    "torch.save(policy, './models/drone_policy_full_v1.4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e60be",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0578c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load state dict into a new model instance\n",
    "policy = DroneGamerBoi().to(device)\n",
    "policy.load_state_dict(torch.load('./models/drone_policy_v1.4.pth'))\n",
    "policy.eval()  # Set to evaluation mode\n",
    "# # Option 2: Load complete model\n",
    "# policy_from_file = torch.load('drone_policy_full.pt')\n",
    "# policy_from_file.eval()  # Set to evaluation mode\n",
    "\n",
    "# # Use policy_from_file or policy_from_state as your loaded model\n",
    "# policy = policy_from_file  # Choose which loaded version to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(client, policy, max_steps=300, game_id=0, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate policy on a single game without training.\n",
    "    \n",
    "    Args:\n",
    "        client (DroneGameClient): Game client instance\n",
    "        policy (DroneGamerBoi): Policy network\n",
    "        max_steps (int): Maximum steps per episode\n",
    "        game_id (int): ID of the game to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Episode statistics including rewards, steps, and outcome\n",
    "    \"\"\"\n",
    "    # Set policy to evaluation mode\n",
    "    policy.eval()\n",
    "    \n",
    "    # Initialize episode\n",
    "    state = client.reset(game_id)\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Run episode\n",
    "    while not done and steps < max_steps:\n",
    "        # Get action probabilities from policy\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            action_probs = policy(state)\n",
    "            \n",
    "        if temperature == 0:\n",
    "            action = (action_probs > 0.5).float()\n",
    "        else:\n",
    "            adjusted_probs = torch.pow(action_probs, 1.0 / temperature)\n",
    "            adjusted_probs = adjusted_probs / (adjusted_probs + torch.pow(1 - action_probs, 1.0 / temperature))\n",
    "            # Sample action from probabilities\n",
    "            dist = Bernoulli(probs=adjusted_probs)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, _, done, _ = client.step({\n",
    "            \"main_thrust\": int(action[0]),\n",
    "            \"left_thrust\": int(action[1]),\n",
    "            \"right_thrust\": int(action[2])\n",
    "        }, game_id)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = calc_reward(next_state)\n",
    "        total_reward += reward['total']\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Update state and step counter\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "    # Return episode statistics\n",
    "    return {\n",
    "        'total_reward': total_reward,\n",
    "        'rewards': rewards,\n",
    "        'steps': steps,\n",
    "        'landed': state.landed,\n",
    "        'crashed': state.crashed,\n",
    "        'final_fuel': state.drone_fuel\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe83278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accumulated_rewards(results, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot accumulated rewards for each component over time.\n",
    "    \n",
    "    Args:\n",
    "        results: Output from evaluate_policy() containing 'rewards' list\n",
    "        figsize: Figure size tuple (width, height)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract reward components\n",
    "    reward_dicts = results['rewards']\n",
    "    steps = len(reward_dicts)\n",
    "    \n",
    "    # Get all component keys (exclude 'total')\n",
    "    components = [key for key in reward_dicts[0].keys() if key != 'total']\n",
    "    \n",
    "    # Initialize accumulated rewards\n",
    "    accumulated = {comp: [] for comp in components}\n",
    "    accumulated['total'] = []\n",
    "    \n",
    "    # Calculate accumulated rewards for each component\n",
    "    for comp in components:\n",
    "        cumsum = 0\n",
    "        for reward_dict in reward_dicts:\n",
    "            cumsum += reward_dict[comp]\n",
    "            accumulated[comp].append(cumsum)\n",
    "    \n",
    "    # Calculate accumulated total\n",
    "    cumsum_total = 0\n",
    "    for reward_dict in reward_dicts:\n",
    "        cumsum_total += reward_dict['total']\n",
    "        accumulated['total'].append(cumsum_total)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: All components separately\n",
    "    ax1 = axes[0]\n",
    "    for comp in components:\n",
    "        ax1.plot(accumulated[comp], label=comp, linewidth=2)\n",
    "    \n",
    "    ax1.set_title('Accumulated Reward by Component', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Time Steps')\n",
    "    ax1.set_ylabel('Accumulated Reward')\n",
    "    ax1.legend(loc='best', framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Total accumulated reward\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(accumulated['total'], color='black', linewidth=3, label='Total')\n",
    "    ax2.set_title('Total Accumulated Reward', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Time Steps')\n",
    "    ax2.set_ylabel('Accumulated Reward')\n",
    "    ax2.legend(loc='best', framealpha=0.9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf06d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = client.reset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_reward(client.get_state(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    plot_accumulated_rewards(evaluate_policy(client, policy, max_steps=1000, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_reward(client.get_state(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_live(client, policy, max_steps=300, game_id=0, temperature=0.5, \n",
    "                         update_interval=10, figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Evaluate policy with live plotting in Jupyter notebook.\n",
    "    \n",
    "    Args:\n",
    "        client (DroneGameClient): Game client instance\n",
    "        policy (DroneGamerBoi): Policy network\n",
    "        max_steps (int): Maximum steps per episode\n",
    "        game_id (int): ID of the game to evaluate\n",
    "        temperature (float): Sampling temperature (0 = greedy)\n",
    "        update_interval (int): Update plot every N steps\n",
    "        figsize (tuple): Figure size\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Set policy to evaluation mode\n",
    "    policy.eval()\n",
    "    \n",
    "    # Initialize episode\n",
    "    state = client.reset(game_id)\n",
    "    \n",
    "    # Storage for plotting\n",
    "    history = {\n",
    "        'step': [],\n",
    "        'distance': [],\n",
    "        'horizontal': [],\n",
    "        'angle': [],\n",
    "        'speed': [],\n",
    "        'corridor': [],\n",
    "        'accumulated_total': [],\n",
    "        'drone_x': [],\n",
    "        'drone_y': [],\n",
    "        'platform_x': [],\n",
    "        'platform_y': [],\n",
    "        'drone_angle': [],\n",
    "        'fuel': [],\n",
    "    }\n",
    "    \n",
    "    accumulated_total = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Run episode with live updates\n",
    "    while not done and steps < max_steps:\n",
    "        # Get action\n",
    "        with torch.no_grad():\n",
    "            action_probs = policy(state)\n",
    "            \n",
    "        if temperature == 0:\n",
    "            action = (action_probs > 0.5).float()\n",
    "        else:\n",
    "            adjusted_probs = torch.pow(action_probs, 1.0 / temperature)\n",
    "            adjusted_probs = adjusted_probs / (adjusted_probs + torch.pow(1 - action_probs, 1.0 / temperature))\n",
    "            dist = Bernoulli(probs=adjusted_probs)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Take action\n",
    "        next_state, _, done, _ = client.step({\n",
    "            \"main_thrust\": int(action[0]),\n",
    "            \"left_thrust\": int(action[1]),\n",
    "            \"right_thrust\": int(action[2])\n",
    "        }, game_id)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = calc_reward(next_state)\n",
    "        accumulated_total += reward['total']\n",
    "        \n",
    "        # Store history\n",
    "        history['step'].append(steps)\n",
    "        history['distance'].append(reward['distance'])\n",
    "        history['horizontal'].append(reward['horizontal'])\n",
    "        history['angle'].append(reward['angle'])\n",
    "        history['speed'].append(reward['speed'])\n",
    "        history['corridor'].append(reward['corridor'])\n",
    "        history['accumulated_total'].append(accumulated_total)\n",
    "        history['drone_x'].append(next_state.drone_x)\n",
    "        history['drone_y'].append(next_state.drone_y)\n",
    "        history['platform_x'].append(next_state.platform_x)\n",
    "        history['platform_y'].append(next_state.platform_y)\n",
    "        history['drone_angle'].append(next_state.drone_angle * 180)\n",
    "        history['fuel'].append(next_state.drone_fuel)\n",
    "        \n",
    "        # Update plots at intervals\n",
    "        if steps % update_interval == 0 or done or steps == max_steps - 1:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Create figure\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            # Trajectory plot\n",
    "            ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
    "            ax1.plot(history['drone_x'], history['drone_y'], 'b-', linewidth=2, alpha=0.6, label='Path')\n",
    "            ax1.plot(next_state.drone_x, next_state.drone_y, 'ro', markersize=15, label='Drone', zorder=5)\n",
    "            \n",
    "            # Draw platform as rectangle\n",
    "            platform_width = 0.125  # 100px / 800px\n",
    "            platform_height = 0.033  # 20px / 600px\n",
    "            platform_rect = plt.Rectangle(\n",
    "                (next_state.platform_x - platform_width/2, next_state.platform_y - platform_height/2),\n",
    "                platform_width, platform_height,\n",
    "                color='green', alpha=0.7, label='Platform'\n",
    "            )\n",
    "            ax1.add_patch(platform_rect)\n",
    "            \n",
    "            # Draw drone orientation arrow\n",
    "            arrow_length = 0.05\n",
    "            import numpy as np\n",
    "            angle_rad = np.radians(next_state.drone_angle * 180)\n",
    "            dx_arrow = arrow_length * np.sin(angle_rad)\n",
    "            dy_arrow = arrow_length * np.cos(angle_rad)\n",
    "            ax1.arrow(next_state.drone_x, next_state.drone_y, dx_arrow, dy_arrow,\n",
    "                     head_width=0.02, head_length=0.01, fc='red', ec='red', zorder=6)\n",
    "            \n",
    "            ax1.set_xlim(0, 1)\n",
    "            ax1.set_ylim(0, 1)\n",
    "            ax1.invert_yaxis()\n",
    "            ax1.set_xlabel('X Position', fontsize=10)\n",
    "            ax1.set_ylabel('Y Position', fontsize=10)\n",
    "            ax1.set_title('Drone Trajectory', fontweight='bold', fontsize=12)\n",
    "            ax1.legend(loc='upper right', fontsize=9)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.set_aspect('equal')\n",
    "            \n",
    "            # Total reward plot\n",
    "            ax2 = fig.add_subplot(gs[0, 2])\n",
    "            ax2.plot(history['step'], history['accumulated_total'], 'k-', linewidth=2)\n",
    "            ax2.set_xlabel('Steps', fontsize=9)\n",
    "            ax2.set_ylabel('Reward', fontsize=9)\n",
    "            ax2.set_title('Total Accumulated Reward', fontweight='bold', fontsize=10)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # State info\n",
    "            ax3 = fig.add_subplot(gs[1, 2])\n",
    "            ax3.axis('off')\n",
    "            state_text = (\n",
    "                f\"{'='*25}\\n\"\n",
    "                f\"STEP: {steps}/{max_steps}\\n\"\n",
    "                f\"{'='*25}\\n\"\n",
    "                f\"Distance:    {next_state.distance_to_platform:.3f}\\n\"\n",
    "                f\"Horizontal:  {abs(next_state.dx_to_platform):.3f}\\n\"\n",
    "                f\"Angle:       {next_state.drone_angle * 180:.1f}°\\n\"\n",
    "                f\"Speed:       {next_state.speed:.3f}\\n\"\n",
    "                f\"Fuel:        {next_state.drone_fuel:.1%}\\n\"\n",
    "                f\"Reward:      {accumulated_total:.1f}\\n\"\n",
    "                f\"\\n{'='*25}\\n\"\n",
    "                f\"ACTIONS:\\n\"\n",
    "                f\"{'='*25}\\n\"\n",
    "                f\"Main:  {'■' if int(action[0]) else '□'}\\n\"\n",
    "                f\"Left:  {'■' if int(action[1]) else '□'}\\n\"\n",
    "                f\"Right: {'■' if int(action[2]) else '□'}\\n\"\n",
    "            )\n",
    "            ax3.text(0.05, 0.95, state_text, transform=ax3.transAxes,\n",
    "                    verticalalignment='top', fontfamily='monospace', fontsize=9,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "            \n",
    "            # Reward components\n",
    "            ax4 = fig.add_subplot(gs[2, :])\n",
    "            ax4.plot(history['step'], history['distance'], label='distance', linewidth=2)\n",
    "            ax4.plot(history['step'], history['horizontal'], label='horizontal', linewidth=2)\n",
    "            ax4.plot(history['step'], history['angle'], label='angle', linewidth=2)\n",
    "            ax4.plot(history['step'], history['speed'], label='speed', linewidth=2)\n",
    "            ax4.plot(history['step'], history['corridor'], label='corridor', linewidth=2)\n",
    "            ax4.set_xlabel('Steps', fontsize=10)\n",
    "            ax4.set_ylabel('Reward Value', fontsize=10)\n",
    "            ax4.set_title('Reward Components Over Time', fontweight='bold', fontsize=12)\n",
    "            ax4.legend(loc='upper left', fontsize=9, ncol=5)\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add status if done\n",
    "            if done or steps >= max_steps - 1:\n",
    "                status_text = \"✅ LANDED!\" if next_state.landed else \"❌ CRASHED\"\n",
    "                status_color = 'green' if next_state.landed else 'red'\n",
    "                fig.text(0.5, 0.95, status_text, ha='center', fontsize=20, \n",
    "                        fontweight='bold', color=status_color,\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EPISODE SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Result: {'LANDED' if state.landed else 'CRASHED'}\")\n",
    "    print(f\"Steps: {steps}/{max_steps}\")\n",
    "    print(f\"Total Reward: {accumulated_total:.2f}\")\n",
    "    print(f\"Final Distance: {state.distance_to_platform:.3f}\")\n",
    "    print(f\"Final Speed: {state.speed:.3f}\")\n",
    "    print(f\"Final Angle: {state.drone_angle * 180:.1f}°\")\n",
    "    print(f\"Fuel Remaining: {state.drone_fuel:.1%}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'history': history,\n",
    "        'total_reward': accumulated_total,\n",
    "        'steps': steps,\n",
    "        'landed': state.landed,\n",
    "        'crashed': state.crashed,\n",
    "        'final_fuel': state.drone_fuel\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a70229",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_policy_live(\n",
    "    client, \n",
    "    policy, \n",
    "    max_steps=500, temperature=0.1, \n",
    "    update_interval=10)\n",
    "# plot_accumulated_rewards(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
