{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6ed65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "from delivery_drone.game.socket_client import DroneGameClient, DroneState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacda758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to localhost:5555...\n",
      "Connected to server at localhost:5555\n",
      "Server has 1 game instance(s)\n"
     ]
    }
   ],
   "source": [
    "client = DroneGameClient()\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbb8f2",
   "metadata": {},
   "source": [
    "### Going from `state` -> `reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6197006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scaler(value, scaler):\n",
    "    # Does value * scaler\n",
    "    return value * scaler\n",
    "\n",
    "def gausian_scaler(value, sigma=0.1, scaler=1):\n",
    "    return scaler * math.exp(-value**2/(2*sigma**2))\n",
    "\n",
    "def exponential_decay(value, decay=10, scaler=1):\n",
    "    return scaler * math.exp(-decay*(abs(value)))\n",
    "\n",
    "def inverse_quadratic(value, decay=10, scaler=1):\n",
    "    return scaler * (1/(1+(decay*value**2)))\n",
    "\n",
    "def inverse_linear(value, decay=10, scaler=1):\n",
    "    return scaler * (1/(1+(decay*abs(value))))\n",
    "\n",
    "def scaled_shifted_negative_sigmoid(value, sigma=10, scaler=1):\n",
    "    return scaler * -1 * (1/(1+math.exp(sigma*(value-0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd75f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(state: DroneState):\n",
    "    reward = 0\n",
    "    \n",
    "    # 1. Time penalty - make hovering expensive\n",
    "    reward -= 0.25\n",
    "    \n",
    "    # 2. Velocity alignment\n",
    "    velocity_dot = (state.drone_vx * state.dx_to_platform + \n",
    "                   state.drone_vy * state.dy_to_platform)\n",
    "    \n",
    "    if velocity_dot > 0:\n",
    "        reward += velocity_dot * 2.0\n",
    "    else:\n",
    "        reward -= abs(velocity_dot) * 0.5\n",
    "    \n",
    "    # 3. Angle penalty - target upright (landing requires |angle| < 20° = 0.111 normalized)\n",
    "    angle_penalty = -(abs(state.drone_angle) ** 2) * 0.3\n",
    "    reward += angle_penalty\n",
    "    \n",
    "    # 4. Angular velocity penalty\n",
    "    angular_vel_penalty = -(abs(state.drone_angular_vel) ** 2) * 0.3\n",
    "    reward += angular_vel_penalty\n",
    "    \n",
    "    # 5. CRITICAL: Horizontal alignment reward\n",
    "    # Platform width is 100px = 0.125 normalized, so must be within ±0.0625\n",
    "    if state.distance_to_platform < 0.2:\n",
    "        abs_dx = abs(state.dx_to_platform)\n",
    "        \n",
    "        if abs_dx < 0.06:  # Within platform horizontally!\n",
    "            # Give HUGE bonus for being horizontally aligned\n",
    "            horizontal_bonus = (0.06 - abs_dx) * 50.0  # Up to +3.0\n",
    "            reward += horizontal_bonus\n",
    "            \n",
    "            # Extra bonus if also stable\n",
    "            if abs(state.drone_angle) < 0.11 and state.speed < 0.3:\n",
    "                reward += 5.0  # Ready to land!\n",
    "        else:\n",
    "            # Outside platform - penalty for being close but misaligned\n",
    "            reward -= 2.0\n",
    "    \n",
    "    # 6. Speed control - landing requires speed < 0.3 (normalized)\n",
    "    if state.distance_to_platform < 0.15:\n",
    "        if state.speed > 0.3:\n",
    "            # Too fast for landing distance\n",
    "            speed_penalty = -(state.speed - 0.3) ** 2 * 10.0\n",
    "            reward += speed_penalty\n",
    "    \n",
    "    # 7. Hovering penalty\n",
    "    if state.speed < 0.15:\n",
    "        reward -= 2.5  # Strong anti-hovering\n",
    "    \n",
    "    # 8. Terminal rewards\n",
    "    if state.landed:\n",
    "        reward += 500.0  # MASSIVE landing reward\n",
    "        reward += state.drone_fuel * 100.0\n",
    "    \n",
    "    if state.crashed:\n",
    "        reward -= 30.0\n",
    "    \n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a997f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = client.reset(0)\n",
    "calc_reward(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975e26c",
   "metadata": {},
   "source": [
    "### Going from `reward` -> `loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ae5af",
   "metadata": {},
   "source": [
    "There are several approaches to convert RL rewards into a loss function for neural networks:\n",
    "1. Policy Gradient Methods (REINFORCE, PPO, A3C)\n",
    "Maximize expected reward by minimizing negative log-likelihood weighted by returns:\n",
    "```py\n",
    "loss = -log_prob(action) * reward\n",
    "# Or with advantage:\n",
    "loss = -log_prob(action) * advantage\n",
    "```\n",
    "2. Q-Learning / DQN\n",
    "Minimize TD (Temporal Difference) error:\n",
    "```py\n",
    "# Predict Q-value for action taken\n",
    "q_predicted = model(state)[action]\n",
    "\n",
    "# Target Q-value (Bellman equation)\n",
    "q_target = reward + gamma * max(model(next_state))\n",
    "\n",
    "# MSE loss\n",
    "loss = (q_predicted - q_target)^2\n",
    "```\n",
    "3. Actor-Critic Methods (A2C, SAC)\n",
    "Two separate losses:\n",
    "```py\n",
    "# Actor loss (policy)\n",
    "actor_loss = -log_prob(action) * advantage\n",
    "\n",
    "# Critic loss (value function)\n",
    "critic_loss = (value_predicted - value_target)^2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771feb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_prob_loss(action_probs, action_index, reward):\n",
    "    \"\"\"\n",
    "    Computes the negative log-probability loss for policy gradient methods.\n",
    "\n",
    "    Args:\n",
    "        action_probs (torch.Tensor): Tensor of probabilities for each action.\n",
    "        action_index (int): Index of the action taken.\n",
    "        reward (float): Scalar reward. Can be replaced by advantage for advantage-based methods.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = -torch.log(action_probs[action_index]) * reward\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927a7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_loss(q_predicted, reward, gamma, q_next_state, done):\n",
    "    \"\"\"\n",
    "    Computes the Q-learning loss using TD error.\n",
    "    \n",
    "    Args:\n",
    "        q_predicted (torch.Tensor): Predicted Q-value for the taken action\n",
    "        reward (float): Immediate reward received\n",
    "        gamma (float): Discount factor for future rewards\n",
    "        q_next_state (torch.Tensor): Predicted Q-values for next state\n",
    "        done (bool): Whether the episode has ended\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The computed TD error loss\n",
    "    \"\"\"\n",
    "    # If done, next state value is 0, otherwise it's the max Q-value of next state\n",
    "    next_value = 0 if done else torch.max(q_next_state)\n",
    "    \n",
    "    # Compute target Q-value using Bellman equation\n",
    "    q_target = reward + gamma * next_value\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    loss = (q_predicted - q_target.detach()) ** 2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4eb369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drone_x': 0.5,\n",
       " 'drone_y': 0.16666666666666666,\n",
       " 'drone_vx': 0.0,\n",
       " 'drone_vy': 0.0,\n",
       " 'drone_angle': 0.0,\n",
       " 'drone_angular_vel': 0.0,\n",
       " 'drone_fuel': 1.0,\n",
       " 'platform_x': 0.36,\n",
       " 'platform_y': 0.81,\n",
       " 'distance_to_platform': 0.502400487658999,\n",
       " 'dx_to_platform': -0.14,\n",
       " 'dy_to_platform': 0.6433333333333333,\n",
       " 'speed': 0.0,\n",
       " 'landed': False,\n",
       " 'crashed': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-2.75"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = client.get_state(0)\n",
    "display(state.__dict__)\n",
    "\n",
    "calc_reward(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a2652",
   "metadata": {},
   "source": [
    "## Let's Create a Policy Network now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b282ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_array(state):\n",
    "    \"\"\"Convert DroneState dataclass to numpy array\"\"\"\n",
    "    data = np.array([\n",
    "        state.drone_x,\n",
    "        state.drone_y,\n",
    "        state.drone_vx,\n",
    "        state.drone_vy,\n",
    "        state.drone_angle,\n",
    "        state.drone_angular_vel,\n",
    "        state.drone_fuel,\n",
    "        state.platform_x,\n",
    "        state.platform_y,\n",
    "        state.distance_to_platform,\n",
    "        state.dx_to_platform,\n",
    "        state.dy_to_platform,\n",
    "        state.speed,\n",
    "        float(state.landed),\n",
    "        float(state.crashed)\n",
    "    ])\n",
    "    \n",
    "    return torch.tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8608438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneGamerBoi(nn.Module):\n",
    "    def __init__(self, state_dim=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.LayerNorm(128),  # Add normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, DroneState):\n",
    "            state = state_to_array(state)\n",
    "        \n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88b404",
   "metadata": {},
   "source": [
    "## How do I train my policy:\n",
    "\n",
    "1. simple approach - Online Learning (Naive):\n",
    "```py\n",
    "state = Reset_game()\n",
    "for _ in range(training_steps):\n",
    "    probs = policy(state)\n",
    "    action = sample_from(probs)\n",
    "    state = game_update(action)\n",
    "    reward = calc_reward\n",
    "    loss = loss_fn(reward)\n",
    "    gradient_step(policy, loss)\n",
    "```\n",
    "**Problems**:\n",
    "- Too much Varience\n",
    "- Our policy will learn to do erratic movements\n",
    "\n",
    "(Not going to implement this BS)\n",
    "\n",
    "2. Episodes - (Less, but still, naive):\n",
    "**Core Idea**: _Take one episode, i.e, let the policy sample till the episode ends, which means either the drone crashed or landed._\n",
    "\n",
    "```py\n",
    "for _ in range(num_training_episode):\n",
    "    # Collect full episode`\n",
    "    states, actions, rewards = [], [], []\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        probs = policy(state)\n",
    "        action = sample_from(probs)\n",
    "        state = game_update(action)\n",
    "\n",
    "        reward = calc_reward(state)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    total_loss = sum([loss_fn(reward) for reward in rewards])\n",
    "    gradient_step(policy, total_loss)\n",
    "```\n",
    "\n",
    "-> This is way better than the previous one, because we will at least optimize for winning (or, negatively, for losing) the game\n",
    "\n",
    "**Problems**: _Still high varience, the policy may learn how to win, but it may also reinforce erratic behaviour_\n",
    "\n",
    "---\n",
    "\n",
    "My point is that there are many ways to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b6eb8",
   "metadata": {},
   "source": [
    "#### Policy Gradient with Baseline\n",
    "\n",
    "**Core Idea**:\n",
    "\n",
    "1. Collect multiple episodes\n",
    "2. Calculate the mean of all the returns (called _Baseline_)\n",
    "3. Calculate advantage of each episode (i.e., subtract all episode's returns with the baseline)\n",
    "4. Compute Loss of the actions weighted against the advantage.\n",
    "5. Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d4f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DroneGamerBoi() # initialize our policy\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48faf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configurations\n",
    "num_iterations = 1000\n",
    "num_episodes = client.num_games\n",
    "\n",
    "bellman_gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd29a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(client: DroneGameClient, policy: DroneGamerBoi, max_steps=300):\n",
    "    \"\"\"\n",
    "    Collect episodes with early stopping\n",
    "    \n",
    "    Args:\n",
    "        max_steps: Maximum steps per episode (default: 300)\n",
    "    \"\"\"\n",
    "    num_games = client.num_games\n",
    "    \n",
    "    # Initialize storage\n",
    "    all_episodes = [{'states': [], 'actions': [], 'log_probs': [], 'rewards': [], 'done': False} \n",
    "                    for _ in range(num_games)]\n",
    "    \n",
    "    # Reset all games\n",
    "    game_states = [client.reset(game_id) for game_id in range(num_games)]\n",
    "    step_counts = [0] * num_games  # Track steps per game\n",
    "    \n",
    "    while not all(ep['done'] for ep in all_episodes):\n",
    "        # Batch active games\n",
    "        batch_states = []\n",
    "        active_game_ids = []\n",
    "        \n",
    "        for game_id in range(num_games):\n",
    "            if not all_episodes[game_id]['done']:\n",
    "                batch_states.append(state_to_array(game_states[game_id]))\n",
    "                active_game_ids.append(game_id)\n",
    "        \n",
    "        if len(batch_states) == 0:\n",
    "            break\n",
    "        \n",
    "        # Batched inference\n",
    "        batch_states_tensor = torch.stack(batch_states)\n",
    "        batch_action_probs = policy(batch_states_tensor)\n",
    "        batch_dist = Bernoulli(probs=batch_action_probs)\n",
    "        batch_actions = batch_dist.sample()\n",
    "        batch_log_probs = batch_dist.log_prob(batch_actions).sum(dim=1)\n",
    "        \n",
    "        # Execute actions\n",
    "        for i, game_id in enumerate(active_game_ids):\n",
    "            action = batch_actions[i]\n",
    "            log_prob = batch_log_probs[i]\n",
    "            \n",
    "            next_state, _, done, _ = client.step({\n",
    "                \"main_thrust\": int(action[0]),\n",
    "                \"left_thrust\": int(action[1]),\n",
    "                \"right_thrust\": int(action[2])\n",
    "            }, game_id)\n",
    "            \n",
    "            reward = calc_reward(next_state)\n",
    "            \n",
    "            # Store data\n",
    "            all_episodes[game_id]['states'].append(batch_states[i])\n",
    "            all_episodes[game_id]['actions'].append(action)\n",
    "            all_episodes[game_id]['log_probs'].append(log_prob)\n",
    "            all_episodes[game_id]['rewards'].append(reward)\n",
    "            \n",
    "            # Update state and step count\n",
    "            game_states[game_id] = next_state\n",
    "            step_counts[game_id] += 1\n",
    "            \n",
    "            # Check done conditions\n",
    "            if done or step_counts[game_id] >= max_steps:\n",
    "                # Apply timeout penalty if hit max steps without landing\n",
    "                if step_counts[game_id] >= max_steps and not next_state.landed:\n",
    "                    all_episodes[game_id]['rewards'][-1] -= 75  # Timeout penalty\n",
    "                \n",
    "                all_episodes[game_id]['done'] = True\n",
    "    \n",
    "    # Return episodes\n",
    "    return [(ep['states'], ep['actions'], ep['log_probs'], ep['rewards']) \n",
    "            for ep in all_episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d50e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns (G_t) for each timestep based on the Bellman equation\n",
    "    \n",
    "    G_t = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Compute backwards (more efficient)\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875b6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steepness = 0.65\n",
    "start = 75\n",
    "end = 250\n",
    "\n",
    "x = np.linspace(0, 1, num=num_iterations)\n",
    "step_schedule = np.round(start + (end - start) * x**steepness).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa23c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "tqdm_iterations = trange(num_iterations, desc='', total=num_iterations)\n",
    "# tqdm_episodes = trange(num_episodes, desc=f'Episode (0/{num_episodes})', total=num_episodes)\n",
    "\n",
    "test_round_at = 5\n",
    "\n",
    "for iteration in tqdm_iterations:\n",
    "    # Collect from all games once (batched)\n",
    "    max_steps = step_schedule[iteration]\n",
    "    \n",
    "    episodes = collect_episodes(client, policy, max_steps=max_steps)\n",
    "    \n",
    "    batch_log_probs = []\n",
    "    batch_returns = []\n",
    "    total_reward = 0\n",
    "    episode_lengths = []  # Track this!\n",
    "    num_successes = 0  # Track this!\n",
    "    \n",
    "    # Process all episodes\n",
    "    for states, actions, log_probs, rewards in episodes:\n",
    "        returns = compute_returns(rewards, gamma=bellman_gamma)\n",
    "        batch_log_probs.extend(log_probs)\n",
    "        batch_returns.extend(returns)\n",
    "        total_reward += sum(rewards)\n",
    "        episode_lengths.append(len(rewards))\n",
    "        # Check if episode succeeded\n",
    "        if rewards[-1] > 0:  # Last reward was positive (landed)\n",
    "            num_successes += 1\n",
    "    \n",
    "    # Train\n",
    "    returns_tensor = torch.tensor(batch_returns, dtype=torch.float32)\n",
    "    baseline = returns_tensor.mean()\n",
    "    advantages = (returns_tensor - baseline)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    log_probs_tensor = torch.stack(batch_log_probs)\n",
    "    loss = -(log_probs_tensor * advantages).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    tqdm_iterations.set_description(\n",
    "        f'Success: {num_successes}/{len(episodes)} | '  # Success rate!\n",
    "        f'Baseline: {baseline.item():.1f} | '\n",
    "        f'Reward Std: {returns_tensor.std():.1f} | '  # Variance in returns\n",
    "        f'Avg Len: {sum(episode_lengths)/len(episode_lengths):.1f} | '\n",
    "        f'Loss: {loss.item():.4f} | '\n",
    "        f'Max Steps: {max_steps}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(policy.state_dict(), 'models/drone_policy.pth')\n",
    "\n",
    "# Optionally, save the entire model (includes architecture)\n",
    "torch.save(policy, 'models/drone_policy_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0578c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/drone_policy.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Option 1: Load state dict into a new model instance\u001b[39;00m\n\u001b[32m      2\u001b[39m policy_from_state = DroneGamerBoi()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m policy_from_state.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/drone_policy.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      4\u001b[39m policy_from_state.eval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# # Option 2: Load complete model\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# policy_from_file = torch.load('drone_policy_full.pt')\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# policy_from_file.eval()  # Set to evaluation mode\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# # Use policy_from_file or policy_from_state as your loaded model\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# policy = policy_from_file  # Choose which loaded version to use\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/drone_policy.pth'"
     ]
    }
   ],
   "source": [
    "# Option 1: Load state dict into a new model instance\n",
    "policy_from_state = DroneGamerBoi()\n",
    "policy_from_state.load_state_dict(torch.load('delivery_drone/models/drone_policy.pth'))\n",
    "policy_from_state.eval()  # Set to evaluation mode\n",
    "\n",
    "# # Option 2: Load complete model\n",
    "# policy_from_file = torch.load('drone_policy_full.pt')\n",
    "# policy_from_file.eval()  # Set to evaluation mode\n",
    "\n",
    "# # Use policy_from_file or policy_from_state as your loaded model\n",
    "# policy = policy_from_file  # Choose which loaded version to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    states, actions, log_probs, rewards = collect_episodes(client, policy_from_state, max_steps=300)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = client.get_state(0)\n",
    "display(state.__dict__)\n",
    "calc_reward(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
